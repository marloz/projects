{
  
    
        "post0": {
            "title": "MNIST fastai walkthrough",
            "content": "Imports . import numpy as np import pandas as pd from pathlib import Path from IPython.core.display import display, HTML import math import matplotlib.pyplot as plt import functools import pylab from fastai.datasets import untar_data, URLs from fastai.vision.image import * from fastai.metrics import accuracy import torch from torch import nn from torchvision import transforms from torch.utils.data import DataLoader from torch import tensor from torch.optim import SGD . %load_ext autoreload %autoreload 2 %matplotlib inline pylab.rcParams[&#39;figure.figsize&#39;] = (15, 8) pylab.rcParams[&#39;font.size&#39;] = 10 display(HTML(&quot;&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;&quot;)) pd.set_option(&#39;display.max_columns&#39;, 100) pd.set_option(&#39;display.max_rows&#39;, 100) pd.set_option(&#39;display.max_colwidth&#39;, -1) pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.2f&#39; % x) . Inspect data . Get data from pre-stored link in one of fastai modules and inspect the content . URLs.MNIST . &#39;https://s3.amazonaws.com/fast-ai-imageclas/mnist_png&#39; . data_path = untar_data(URLs.MNIST) data_path . PosixPath(&#39;/root/.fastai/data/mnist_png&#39;) . Dataset is already split into testing and training . data_path.ls() . [PosixPath(&#39;/root/.fastai/data/mnist_png/testing&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training&#39;)] . Each has separate folders for target classes 0 - 9 . Path.joinpath(data_path, &#39;testing&#39;).ls() . [PosixPath(&#39;/root/.fastai/data/mnist_png/testing/5&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/6&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/4&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/1&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/9&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/7&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/3&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/2&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/8&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/testing/0&#39;)] . See number of images in each, around 80/20% split . [f&quot;Number of images in training set {len(Path.joinpath(data_path, &#39;training&#39;, str(label)).ls())}, &quot; f&quot;test set {len(Path.joinpath(data_path, &#39;testing&#39;, str(label)).ls())}&quot; for label in range(10)] . [&#39;Number of images in training set 5923, test set 980&#39;, &#39;Number of images in training set 6742, test set 1135&#39;, &#39;Number of images in training set 5958, test set 1032&#39;, &#39;Number of images in training set 6131, test set 1010&#39;, &#39;Number of images in training set 5842, test set 982&#39;, &#39;Number of images in training set 5421, test set 892&#39;, &#39;Number of images in training set 5918, test set 958&#39;, &#39;Number of images in training set 6265, test set 1028&#39;, &#39;Number of images in training set 5851, test set 974&#39;, &#39;Number of images in training set 5949, test set 1009&#39;] . Each image has identifier in its filename . Path.joinpath(data_path, &#39;training&#39;, &#39;0&#39;).ls()[:10] . [PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/15559.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/55745.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/4340.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/26073.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/20548.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/19492.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/31470.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/8323.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/19009.png&#39;), PosixPath(&#39;/root/.fastai/data/mnist_png/training/0/11409.png&#39;)] . Images are of size 28 x 28 pixels, monochrome . im = PIL.Image.open(&#39;/root/.fastai/data/mnist_png/training/0/15559.png&#39;) print(im.size) im . (28, 28) . From images to tensors . Load image from previous section to tensor, check it&#39;s type and dimensions . im_tensor = transforms.ToTensor()(im) type(im_tensor), im_tensor.shape . (torch.Tensor, torch.Size([1, 28, 28])) . Load few images from a single class into tensor. Note that there&#39;s one extra dimension, so we need to reshape into rank 2 before stacking . zeroes_path = Path.joinpath(data_path, &#39;training&#39;, &#39;0&#39;).ls()[:10] zeroes = torch.stack([transforms.ToTensor()(PIL.Image.open(image_path)).reshape((28, 28)) for image_path in zeroes_path]) . Now this tensor contains first 10 images from zeroes category . print(zeroes.shape) plt.imshow(zeroes[0]) . torch.Size([10, 28, 28]) . &lt;matplotlib.image.AxesImage at 0x7f4dc1d4db00&gt; . Now concatenate tensors loaded from couple categories . class_paths = [Path.joinpath(data_path, &#39;training&#39;, &#39;0&#39;), Path.joinpath(data_path, &#39;training&#39;, &#39;1&#39;)] NUM_SAMPLES = 10 zeroes_and_ones = torch.cat([ torch.stack([transforms.ToTensor()(PIL.Image.open(image_path)).reshape((28, 28)) for image_path in class_path.ls()[:NUM_SAMPLES]]) for class_path in class_paths ]) . Small utitlity to show few images at once . import math def show_images(ims, nrows=1, ncols=None): if ncols is None: ncols = int(math.ceil(len(ims) / nrows)) axs = plt.subplots(nrows, ncols)[1].flat for im, ax in zip(ims, axs): ax.imshow(im) . Now we have images from both classes loaded in a single tensor . print(zeroes_and_ones.shape) show_images((zeroes_and_ones[0], zeroes_and_ones[-1])) . torch.Size([20, 28, 28]) . Input look already scaled, so don&#39;t need to take extra preprocessing steps . zeroes_and_ones.min(), zeroes_and_ones.max() . (tensor(0.), tensor(1.)) . Reshape images into vectors, such that each image is a row / observation, that&#39;s our prepared x data . x = zeroes_and_ones.view(-1, 28 * 28) x.shape . torch.Size([20, 784]) . Load labels to tensor . Extract labels from image folder path and get number of images in each . labels_sizes = [(int(str(path).split(&#39;/&#39;)[-1]), len(path.ls())) for path in class_paths] labels_sizes . [(0, 5923), (1, 6742)] . Repeat each label number of times specified by number of items in class . y = tensor([label for label_size in labels_sizes for label in [label_size[0]] * label_size[1]]) print(f&#39;Target shape as expected: {y.shape[0] == labels_sizes[0][1] + labels_sizes[1][1]}&#39;) y . Target shape as expected: True . tensor([0, 0, 0, ..., 1, 1, 1]) . Customized Image Dataset . Based on findings above, let&#39;s create a single class performing all these steps and returning features (X) and targets (y) as tensors. . class ImageDataSet: def __init__(self, data_path, image_size=(28, 28), num_samples=None): self.data_path = data_path self.image_size = image_size self.num_samples = num_samples def load_class_images_to_tensor_list(self, class_path): return torch.stack([transforms.ToTensor()(PIL.Image.open(image_path)).reshape(self.image_size) for image_path in class_path.ls()[:self.num_samples]]).float() def stack_class_tensors(self): return torch.cat([ self.load_class_images_to_tensor_list(class_path) for class_path in sorted(self.data_path.ls()) ]) @staticmethod def tensor_to_vector(t): return t.view(-1, t.shape[-1] * t.shape[-2]) @property def x(self): stacked_tensor = self.stack_class_tensors() return self.tensor_to_vector(stacked_tensor) @property def y(self): labels_sizes = [ (self._extract_label_from_path(path), len(path.ls()[:self.num_samples])) for path in sorted(self.data_path.ls()) ] return tensor([label for label_size in labels_sizes for label in [label_size[0]] * label_size[1]]) @staticmethod def _extract_label_from_path(path): return int(str(path).split(&#39;/&#39;)[-1]) . Use this class to create training and validation sets with 100 samples each . ds = ImageDataSet(data_path=Path.joinpath(data_path, &#39;training&#39;), num_samples=100) train_x, train_y = ds.x, ds.y ds_val = ImageDataSet(Path.joinpath(data_path, &#39;testing&#39;), num_samples=100) valid_x, valid_y = ds_val.x, ds_val.y . Shapes and images look as expected . print(train_x.shape, train_y.shape) print(train_y[0], train_y[-1]) show_images((train_x[0].view(28, 28), train_x[-1].view(28, 28))) . torch.Size([1000, 784]) torch.Size([1000]) tensor(0) tensor(9) . Zip x and y into single data structures . dset = list(zip(train_x, train_y)) valid_dset = list(zip(valid_x, valid_y)) . Check again that everything is correct . x1, y1 = valid_dset[0] x2, y2 = valid_dset[-1] print(y1, y2) show_images((x1.view(28, 28), x2.view(28, 28))) . tensor(0) tensor(9) . Custom Data Loader . Now let&#39;s pass these created datasets to training and validation data loaders that randomize image order and allow us to stream inputs in batches for stochastic gradient descent . import random class DataLoader_: def __init__(self, dset, batch_size): self.chunked_dset = self.chunker(dset, batch_size) def chunker(self, dset, batch_size): random.shuffle(dset) return (dset[idx:idx + batch_size] for idx in range(0, len(dset), batch_size)) def __iter__(self): return self def __next__(self): try: x_b, y_b = list(zip(*next(self.chunked_dset))) return torch.stack(x_b), torch.stack(y_b) except IndexError: raise StopIteration() . dl = DataLoader_(dset, batch_size=256) . x_b, y_b = next(dl) x_b.shape, y_b.shape . (torch.Size([256, 784]), torch.Size([256])) . Looks like our custom data loader is working fine so far! . plt.imshow(x_b[0].view(28, 28)), y_b[0] . (&lt;matplotlib.image.AxesImage at 0x7f4dc1de24e0&gt;, tensor(0)) . Custom learner . Let&#39;s build a learner that is initialized by passing: . dataloader with batch size | neural net | optimizer | loss function | metric | . Then trains and validates the model for a specified number of epochs in the following manner: . get batch of data from training data loader | perform forward pass through the network | calculate loss and it&#39;s gradients | update weights via backpropagation using optimizer | get batch from validation data loader | perform forward pass through the network | calculate and print performance metric | . class CustomLearner: def __init__(self, dsets, batch_size, model, optimizer, loss_fn, metric): self.train_dset, self.valid_dset = dsets self.batch_size = batch_size self.model = model self.optimizer = optimizer self.loss_fn = loss_fn self.metric = metric def train_model(self, num_epochs): for epoch in range(num_epochs): self.train_epoch() print(self.validate_epoch()) def train_epoch(self): train_dl = DataLoader_(self.train_dset, self.batch_size) for x_batch, y_batch in train_dl: self.calculate_gradient(x_batch, y_batch) self.optimizer.step() self.optimizer.zero_grad() def calculate_gradient(self, x_batch, y_batch): predictions = self.model.forward(x_batch) self.loss_fn(predictions, y_batch).backward() def validate_epoch(self): valid_dl = DataLoader_(self.valid_dset, self.batch_size) metrics = [self.metric(self.model.forward(x_batch), y_batch) for x_batch, y_batch in valid_dl] return torch.stack(metrics).mean().item() . Setting constants that will be used for later customizations. Number of epochs, learning rate, number of activations are set to achieve fast iteration time, in real life more time should be spent finding the right values. . BATCH_SIZE = 256 LEARNING_RATE = .1 NUM_CLASSES = 10 NUM_EPOCHS = 20 NUM_ACTIVATIONS = 30 INPUT_SIZE = np.prod(ds.image_size) dsets = dset, valid_dset . Set up simple neural net with two hidden linear layers and ReLU non-linearity inbetween. SGD optimizer is imported from Torch. . simple_net = nn.Sequential( nn.Linear(INPUT_SIZE, NUM_ACTIVATIONS), nn.ReLU(), nn.Linear(NUM_ACTIVATIONS, NUM_CLASSES) ) optimizer = SGD(simple_net.parameters(), lr=LEARNING_RATE) . Could get close to 90% accuracy, if ran for more epochs, even with 100 samples of each category . CustomLearner(dsets=dsets, model=simple_net, optimizer=optimizer, loss_fn=nn.functional.cross_entropy, metric=accuracy, batch_size=BATCH_SIZE).train_model(NUM_EPOCHS) . 0.2705078125 0.37961339950561523 0.38820043206214905 0.39874058961868286 0.4237270951271057 0.515625 0.6041218042373657 0.6594154238700867 0.7188173532485962 0.7535358667373657 0.7850888967514038 0.7883216738700867 0.8005118370056152 0.8135439157485962 0.8139142990112305 0.8240840435028076 0.8288658261299133 0.8256330490112305 0.8353986740112305 0.8409886956214905 . Custom Neural Net . Reconstruct same neural net used above from scratch (using matrix algebra instead of nn module) . from torch.nn.parameter import Parameter # Still need to import this to simplify weight optimization class SimpleNet(nn.Module): def __init__(self, input_size, num_activations, output_size): super().__init__() self.weights_1, self.bias_1 = (self.init_params((input_size, num_activations)), self.init_params(num_activations)) self.weights_2, self.bias_2 = (self.init_params((num_activations, output_size)), self.init_params(output_size)) @staticmethod def init_params(size): return Parameter(torch.randn(size)) @staticmethod def linear(x_batch, weight, bias): return x_batch.matmul(weight) + bias @staticmethod def relu(x_batch): return x_batch.max(tensor(.0)) def forward(self, x_batch): res = self.linear(x_batch, self.weights_1, self.bias_1) res = self.relu(res) return self.linear(res, self.weights_2, self.bias_2) . For some reason results are not the same as with torch implementation but loss is improving, so let&#39;s leave it for now. . sn = SimpleNet(INPUT_SIZE, NUM_ACTIVATIONS, NUM_CLASSES) optimizer = SGD(sn.parameters(), lr=LEARNING_RATE) CustomLearner(dsets=dsets, model=sn, optimizer=optimizer, loss_fn=nn.functional.cross_entropy, metric=accuracy, batch_size=BATCH_SIZE).train_model(NUM_EPOCHS) . 0.2481479048728943 0.30222925543785095 0.3339506983757019 0.3820379972457886 0.4140961766242981 0.4288456439971924 0.45251885056495667 0.4697939157485962 0.4874057173728943 0.5145474076271057 0.5266702771186829 0.5344827771186829 0.5311826467514038 0.5461341738700867 0.553104817867279 0.5487607717514038 0.5586274266242981 0.5698074102401733 0.5704808831214905 0.575835108757019 . Custom optimizer . Now let&#39;s replace torch SGD with our own, that updates weights by multiplying their respective gradients by the learning rate and resets gradients afterwards for the next batch. . class CustomSGD: def __init__(self, params, learning_rate): self.params = list(params) self.learning_rate = learning_rate def step(self): for p in self.params: p.data -= p.grad.data * self.learning_rate def zero_grad(self): for p in self.params: p.grad = None . Results look similar . sn = SimpleNet(INPUT_SIZE, NUM_ACTIVATIONS, NUM_CLASSES) optimizer = CustomSGD(sn.parameters(), LEARNING_RATE) CustomLearner(dsets=dsets, model=sn, optimizer=optimizer, loss_fn=nn.functional.cross_entropy, metric=accuracy, batch_size=BATCH_SIZE).train_model(NUM_EPOCHS) . 0.18989089131355286 0.2645474076271057 0.32869747281074524 0.37624597549438477 0.4138941168785095 0.4393520951271057 0.46656113862991333 0.47875136137008667 0.4880791902542114 0.5011449456214905 0.5049164891242981 0.5155239701271057 0.5196322798728943 0.5225619673728943 0.5179822444915771 0.5334051847457886 0.5425310134887695 0.5500741004943848 0.5528690814971924 0.5544517636299133 . Custom loss function . Now let&#39;s replace torch&#39;s cross entropy loss. Which essentially applies log-softmax to each row of last activation layer output (recall this output has n_input * n_classes shape) and picks value that corresponds to target class index and takes the negative mean of them. Also note that softmax = exponentiated element of a row divided by the sum of all exponentiated row elements. . Let&#39;s take one batch and pass it through our already trained network to illustrate this . x_b, y_b = next(dl) . preds = sn.forward(x_b) print(preds.shape) preds . torch.Size([256, 10]) . tensor([[ -9.6009, -2.2568, -5.2367, ..., 3.7184, -5.2098, 3.2839], [ -5.9776, 0.3611, -0.2300, ..., 2.1829, 2.4964, 1.5622], [ -9.5683, -4.8592, 0.0759, ..., -4.8130, 1.6519, -2.8534], ..., [ -6.4912, 16.1188, 4.1472, ..., 4.3423, -6.6100, 5.7818], [ -6.3775, 9.4231, 3.0040, ..., 1.6132, -4.2952, 1.6586], [ -0.6388, 10.0356, 5.0243, ..., 8.9645, -18.8030, 2.5508]], grad_fn=&lt;AddBackward0&gt;) . def _log_softmax(input, dim): return input.exp().div(input.exp().sum(dim).unsqueeze(1)).log() . def cross_entropy_loss(preds, target): return -_log_softmax(preds, 1).gather(1, target.unsqueeze(1)).mean() . Differences in log_softmax implementation are small, but for some reason using custom softmax does not update parameters in optimization process . cross_entropy_loss(preds, y_b), nn.functional.cross_entropy(preds, y_b) . (tensor(2.1358, grad_fn=&lt;NegBackward&gt;), tensor(2.1358, grad_fn=&lt;NllLossBackward&gt;)) . sn = SimpleNet(INPUT_SIZE, NUM_ACTIVATIONS, NUM_CLASSES) optimizer = CustomSGD(sn.parameters(), LEARNING_RATE) CustomLearner(dsets=dsets, model=sn, optimizer=optimizer, loss_fn=cross_entropy_loss, metric=accuracy, batch_size=BATCH_SIZE).train_model(NUM_EPOCHS) . 0.09987877309322357 0.09987877309322357 0.09987877309322357 0.09947467595338821 0.09947467595338821 0.09977774322032928 0.0995756983757019 0.09967672824859619 0.10008081793785095 0.10028286278247833 0.10048491507768631 0.09977774322032928 0.1005859375 0.10008081793785095 0.10068695992231369 0.0995756983757019 0.09967672824859619 0.09937365353107452 0.10018184036016464 0.09997979551553726 . But using torch log_softmax works fine . def cross_entropy_loss(preds, target): return -preds.log_softmax(1).gather(1, target.unsqueeze(1)).mean() . sn = SimpleNet(INPUT_SIZE, NUM_ACTIVATIONS, NUM_CLASSES) optimizer = CustomSGD(sn.parameters(), LEARNING_RATE) CustomLearner(dsets=dsets, model=sn, optimizer=optimizer, loss_fn=cross_entropy_loss, metric=accuracy, batch_size=BATCH_SIZE).train_model(NUM_EPOCHS) . 0.16544315218925476 0.2676791548728943 0.34223464131355286 0.3937567472457886 0.42261582612991333 0.4590180516242981 0.4806371331214905 0.494140625 0.5094962120056152 0.5077788233757019 0.5172750353813171 0.535493016242981 0.5441473722457886 0.5463699102401733 0.555327296257019 0.5573814511299133 0.562163233757019 0.5645878314971924 0.569672703742981 0.5773168206214905 . Custom metric . Final and simplest piece of this exercise is getting multi-class accuracy. For each prediction row pick the index of the column that has the highest value, compare if it&#39;s equal to the target and take the mean of all predictions to get accuracy. . def custom_accuracy(preds, target): return (preds.argmax(1).unsqueeze(1) == target.unsqueeze(1)).float().mean() . sn = SimpleNet(INPUT_SIZE, NUM_ACTIVATIONS, NUM_CLASSES) optimizer = CustomSGD(sn.parameters(), LEARNING_RATE) CustomLearner(dsets=dsets, model=sn, optimizer=optimizer, loss_fn=cross_entropy_loss, metric=custom_accuracy, batch_size=BATCH_SIZE).train_model(NUM_EPOCHS) . 0.16419720649719238 0.23895473778247833 0.2821592092514038 0.30627021193504333 0.3289668560028076 0.34718480706214905 0.35779228806495667 0.37570714950561523 0.3868534564971924 0.39109644293785095 0.40079471468925476 0.4024784564971924 0.4102909564971924 0.4180024266242981 0.42042699456214905 0.4383081793785095 0.44554823637008667 0.4532597064971924 0.45140761137008667 0.45942214131355286 . Conclusion . Although there are some bits that didn&#39;t work 100% as expected, this has helped me to better understand what&#39;s happening behind the curtains of fastai and torch and more generally, the basics of how simple neural networks work. I urge everyone to check Jeremy Howard&#39;s fantastic lectures on https://www.fast.ai/ .",
            "url": "https://marloz.github.io/projects/2020/11/10/mnist-fastai-walkthrough.html",
            "relUrl": "/2020/11/10/mnist-fastai-walkthrough.html",
            "date": " • Nov 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Automated feature engineering with featuretools",
            "content": "# pandas and numpy for data manipulation import pandas as pd import numpy as np # featuretools for automated feature engineering # !pip install featuretools --upgrade import featuretools as ft # matplotlit and seaborn for visualizations import matplotlib.pyplot as plt import pylab pylab.rcParams[&#39;figure.figsize&#39;] = (15, 8) pylab.rcParams[&#39;font.size&#39;] = 10 import seaborn as sns # Suppress warnings from pandas import warnings warnings.filterwarnings(&#39;ignore&#39;) . This notebook explores featuretools library for automated feature engineering. I will use Kaggle&#39;s home credit default risk dataset, which has just the right structure for this task. Thanks to Will Koehrsen for awesome walkthroughs and explanations of this package. . Data overview . This dataset has application, previous loan payment and credit data. Since the aim is to understand featuretools, I will not spend too much time digging deep into the data and just pick application, bureau and bureau_balance files for this. . . Application data . # Utility to quickly inspect data def inspect_df(df, target_col=None): print(f&#39;df shape: {df.shape}&#39;) print(&#39;_____________________&#39;) print(f&#39;datatypes: {df.dtypes.value_counts()}&#39;) print(&#39;_____________________&#39;) print(f&#39;Num null vals: {df.isnull().sum().sum()}&#39;) print(&#39;_____________________&#39;) if target_col is not None: print(f&#39;{target_col} classes: n{df[target_col].value_counts()}&#39;) print(&#39;_____________________&#39;) return df.head() . Application data is of considerable size both in terms of rows and columns, has mixed data types, some missing values and inbalanced target classes. Since automated feature generation is quite resource intensive process, we need to downsample, preprocess and reduce dimensionality before creating more features. . app_df = pd.read_csv(&#39;../input/home-credit-default-risk/application_train.csv&#39;) inspect_df(app_df, target_col=&#39;TARGET&#39;) . df shape: (307511, 122) _____________________ datatypes: float64 65 int64 41 object 16 dtype: int64 _____________________ Num null vals: 9152465 _____________________ TARGET classes: 0 282686 1 24825 Name: TARGET, dtype: int64 _____________________ . SK_ID_CURR TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY CNT_CHILDREN AMT_INCOME_TOTAL AMT_CREDIT AMT_ANNUITY ... FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR . 0 100002 | 1 | Cash loans | M | N | Y | 0 | 202500.0 | 406597.5 | 24700.5 | ... | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 1 100003 | 0 | Cash loans | F | N | N | 0 | 270000.0 | 1293502.5 | 35698.5 | ... | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 100004 | 0 | Revolving loans | M | Y | Y | 0 | 67500.0 | 135000.0 | 6750.0 | ... | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 100006 | 0 | Cash loans | F | N | Y | 0 | 135000.0 | 312682.5 | 29686.5 | ... | 0 | 0 | 0 | 0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 100007 | 0 | Cash loans | M | N | Y | 0 | 121500.0 | 513000.0 | 21865.5 | ... | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 122 columns . Balance target classes . # Downsample majority class to have same number of rows as minority class def balance_target(df, target_col, positive_class=1, random_state=42): positive_idx = df[df[target_col] == positive_class].index negative_idx = (df.loc[~df.index.isin(positive_idx)] .sample(len(positive_idx), replace=False, random_state=random_state)).index return df.loc[positive_idx.union(negative_idx)] . # Reduction from 300k rows to 50k should speed up exploration and since classes are balanced, shouldn&#39;t affect the accuracy too much. app_df_sample = balance_target(app_df, target_col=&#39;TARGET&#39;) app_df_sample.TARGET.value_counts() . 1 24825 0 24825 Name: TARGET, dtype: int64 . Preprocessing . # Splitter into features and target def split_x_y(df, target_col): return df.drop(target_col, axis=1), df[target_col] . Create a data preparator that: . separates id columns | splits features into categorical and numerical | fills in missing values | factorizes categories (numerical encoding) | reduces float precision of numericals for faster processing | adds random features as benchmark for feature selection later | . class DataPreparator: def __init__(self, id_cols, add_rand_cols=False): self.id_cols = id_cols self.add_rand_cols = add_rand_cols np.random.seed(42) def prepare_data(self, X, cat_fill_val=&#39;none&#39;, cat_trans_func=lambda x: pd.factorize(x)[0], cat_rand_func=lambda x: np.random.choice([0, 1], x.shape[0]), num_fill_val=0, num_trans_func=lambda x: x.astype(&#39;float32&#39;), num_rand_func=lambda x: np.random.rand(x.shape[0])): ids, X = X[self.id_cols], X.drop(self.id_cols, axis=1) X_cat = self._preprocess(X, &#39;object&#39;, cat_fill_val, cat_trans_func, cat_rand_func) X_num = self._preprocess(X, &#39;number&#39;, num_fill_val, num_trans_func, num_rand_func) return pd.concat([ids, X_cat, X_num], axis=1) def _preprocess(self, X, dtypes, fill_val, trans_func, rand_func): X_proc = (X.select_dtypes(include=dtypes) .fillna(fill_val) .apply(trans_func)) if X_proc.shape[0] &gt; 0: return X_proc.assign(**{f&#39;rand_{dtypes}&#39;: rand_func}) if self.add_rand_cols else X_proc . app_df_feat, y = split_x_y(app_df_sample, target_col=&#39;TARGET&#39;) app_df_proc = DataPreparator(id_cols=[&#39;SK_ID_CURR&#39;], add_rand_cols=True).prepare_data(app_df_feat) inspect_df(app_df_proc) . df shape: (49650, 123) _____________________ datatypes: float32 104 int64 18 float64 1 dtype: int64 _____________________ Num null vals: 0 _____________________ _____________________ . SK_ID_CURR NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY NAME_TYPE_SUITE NAME_INCOME_TYPE NAME_EDUCATION_TYPE NAME_FAMILY_STATUS NAME_HOUSING_TYPE ... FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR rand_number . 0 100002 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.319204 | . 7 100010 | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.431163 | . 13 100017 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.052360 | . 17 100021 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.992538 | . 25 100030 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.507610 | . 5 rows × 123 columns . Inspection shows that no rows are lost, all missing values are filled and all columns are numeric . Reduce number of features . The idea here is to fit simple model with in-built feature importances (Random Forest) and drop all columns that have lower significance than random columns added in preparation step. . from sklearn.ensemble import RandomForestClassifier class FeatureSelector: def __init__(self, X, y, id_cols, rand_cols): self.X = X self.y = y self.id_cols = id_cols self.rand_cols = rand_cols def select_important_features(self): rf = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=-1, random_state=42) rf.fit(self.X, self.y) print(f&#39;Model score with full feature set {rf.oob_score_}&#39;) important_cols = self.get_important_cols(rf, self.X.columns) rf.fit(self.X[important_cols], self.y) print(f&#39;Model score with reduced feature set {rf.oob_score_}&#39;) return self.X[self.id_cols + important_cols] def get_important_cols(self, model, column_names): importances = pd.Series(model.feature_importances_, index=column_names) rand_importance = np.max(importances.loc[importances.index.isin(self.rand_cols)]) important_cols = importances[importances &gt; rand_importance].index.tolist() print(f&#39;Number of features with greater than random column importance {len(important_cols)}&#39;) importances.sort_values().plot(title=&#39;feature importance&#39;) return important_cols . app_df_reduced = FeatureSelector(app_df_proc, y, id_cols=[&#39;SK_ID_CURR&#39;], rand_cols=[&#39;rand_object&#39;, &#39;rand_number&#39;]).select_important_features() inspect_df(app_df_reduced) . Model score with full feature set 0.6591742195367573 Number of features with greater than random column importance 9 Model score with reduced feature set 0.655730110775428 df shape: (49650, 10) _____________________ datatypes: float32 9 int64 1 dtype: int64 _____________________ Num null vals: 0 _____________________ _____________________ . SK_ID_CURR AMT_ANNUITY DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 DAYS_LAST_PHONE_CHANGE . 0 100002 | 24700.5 | -9461.0 | -637.0 | -3648.0 | -2120.0 | 0.083037 | 0.262949 | 0.139376 | -1134.0 | . 7 100010 | 42075.0 | -18850.0 | -449.0 | -4597.0 | -2379.0 | 0.000000 | 0.714279 | 0.540654 | -1070.0 | . 13 100017 | 28966.5 | -14086.0 | -3028.0 | -643.0 | -4911.0 | 0.000000 | 0.566907 | 0.770087 | -4.0 | . 17 100021 | 13500.0 | -9776.0 | -191.0 | -4143.0 | -2427.0 | 0.000000 | 0.683513 | 0.000000 | -2811.0 | . 25 100030 | 11074.5 | -19334.0 | -3494.0 | -2419.0 | -2893.0 | 0.561948 | 0.651406 | 0.461482 | 0.0 | . Neat! We reduced number of features from 122 down to 9 (not counting ID), while model score changed only slightly. . Bureau data . Will need to perform downsampling and preprocessing for other dataframes involved as well. The key is to select only rows that link main dataframe on SK_ID_CURR column. . def sample_from_parent_df(parent_df, id_col, child_df): sample_ids = parent_df.set_index(id_col).index child_df = (child_df.set_index(id_col) .apply(lambda x: x.loc[x.index.isin(sample_ids)]) .reset_index()) print(f&#39;Num ids in parent df: {len(sample_ids)}, &#39; f&#39;num ids in child df: {child_df[id_col].nunique()}&#39;) return child_df . bureau_df = sample_from_parent_df(parent_df=app_df_reduced, id_col=&#39;SK_ID_CURR&#39;, child_df=pd.read_csv(&#39;../input/home-credit-default-risk/bureau.csv&#39;)) inspect_df(bureau_df) . Num ids in parent df: 49650, num ids in child df: 41689 df shape: (232611, 17) _____________________ datatypes: float64 8 int64 6 object 3 dtype: int64 _____________________ Num null vals: 549613 _____________________ _____________________ . SK_ID_CURR SK_ID_BUREAU CREDIT_ACTIVE CREDIT_CURRENCY DAYS_CREDIT CREDIT_DAY_OVERDUE DAYS_CREDIT_ENDDATE DAYS_ENDDATE_FACT AMT_CREDIT_MAX_OVERDUE CNT_CREDIT_PROLONG AMT_CREDIT_SUM AMT_CREDIT_SUM_DEBT AMT_CREDIT_SUM_LIMIT AMT_CREDIT_SUM_OVERDUE CREDIT_TYPE DAYS_CREDIT_UPDATE AMT_ANNUITY . 0 162297 | 5714469 | Closed | currency 1 | -1896 | 0 | -1684.0 | -1710.0 | 14985.0 | 0 | 76878.45 | 0.0 | 0.0 | 0.0 | Consumer credit | -1710 | NaN | . 1 162297 | 5714470 | Closed | currency 1 | -1146 | 0 | -811.0 | -840.0 | 0.0 | 0 | 103007.70 | 0.0 | 0.0 | 0.0 | Consumer credit | -840 | NaN | . 2 162297 | 5714471 | Active | currency 1 | -1146 | 0 | -484.0 | NaN | 0.0 | 0 | 4500.00 | 0.0 | 0.0 | 0.0 | Credit card | -690 | NaN | . 3 162297 | 5714472 | Active | currency 1 | -1146 | 0 | -180.0 | NaN | 0.0 | 0 | 337500.00 | 0.0 | 0.0 | 0.0 | Credit card | -690 | NaN | . 4 162297 | 5714473 | Closed | currency 1 | -2456 | 0 | -629.0 | -825.0 | NaN | 0 | 675000.00 | 0.0 | 0.0 | 0.0 | Consumer credit | -706 | NaN | . Since this table has higher granularity, we cannot perform feature reduction in the same way like we did with application data and it has just 17 columns anyway, also there&#39;s no need to add random features when processing. . bureau_df_proc = DataPreparator(id_cols=[&#39;SK_ID_CURR&#39;, &#39;SK_ID_BUREAU&#39;]).prepare_data(bureau_df) inspect_df(bureau_df_proc) . df shape: (232611, 17) _____________________ datatypes: float32 12 int64 5 dtype: int64 _____________________ Num null vals: 0 _____________________ _____________________ . SK_ID_CURR SK_ID_BUREAU CREDIT_ACTIVE CREDIT_CURRENCY CREDIT_TYPE DAYS_CREDIT CREDIT_DAY_OVERDUE DAYS_CREDIT_ENDDATE DAYS_ENDDATE_FACT AMT_CREDIT_MAX_OVERDUE CNT_CREDIT_PROLONG AMT_CREDIT_SUM AMT_CREDIT_SUM_DEBT AMT_CREDIT_SUM_LIMIT AMT_CREDIT_SUM_OVERDUE DAYS_CREDIT_UPDATE AMT_ANNUITY . 0 162297 | 5714469 | 0 | 0 | 0 | -1896.0 | 0.0 | -1684.0 | -1710.0 | 14985.0 | 0.0 | 76878.453125 | 0.0 | 0.0 | 0.0 | -1710.0 | 0.0 | . 1 162297 | 5714470 | 0 | 0 | 0 | -1146.0 | 0.0 | -811.0 | -840.0 | 0.0 | 0.0 | 103007.703125 | 0.0 | 0.0 | 0.0 | -840.0 | 0.0 | . 2 162297 | 5714471 | 1 | 0 | 1 | -1146.0 | 0.0 | -484.0 | 0.0 | 0.0 | 0.0 | 4500.000000 | 0.0 | 0.0 | 0.0 | -690.0 | 0.0 | . 3 162297 | 5714472 | 1 | 0 | 1 | -1146.0 | 0.0 | -180.0 | 0.0 | 0.0 | 0.0 | 337500.000000 | 0.0 | 0.0 | 0.0 | -690.0 | 0.0 | . 4 162297 | 5714473 | 0 | 0 | 0 | -2456.0 | 0.0 | -629.0 | -825.0 | 0.0 | 0.0 | 675000.000000 | 0.0 | 0.0 | 0.0 | -706.0 | 0.0 | . Bureau balance data . bureau_bal_df = sample_from_parent_df(parent_df=bureau_df_proc, id_col=&#39;SK_ID_BUREAU&#39;, child_df=pd.read_csv(&#39;../input/home-credit-default-risk/bureau_balance.csv&#39;)) inspect_df(bureau_bal_df) . Num ids in parent df: 232611, num ids in child df: 85663 df shape: (2224269, 3) _____________________ datatypes: int64 2 object 1 dtype: int64 _____________________ Num null vals: 0 _____________________ _____________________ . SK_ID_BUREAU MONTHS_BALANCE STATUS . 0 5715565 | 0 | X | . 1 5715565 | -1 | 0 | . 2 5715565 | -2 | 0 | . 3 5715565 | -3 | X | . 4 5715565 | -4 | 0 | . bureau_bal_df_proc = DataPreparator(id_cols=[&#39;SK_ID_BUREAU&#39;]).prepare_data(bureau_bal_df) inspect_df(bureau_bal_df_proc) . df shape: (2224269, 3) _____________________ datatypes: int64 2 float32 1 dtype: int64 _____________________ Num null vals: 0 _____________________ _____________________ . SK_ID_BUREAU STATUS MONTHS_BALANCE . 0 5715565 | 0 | 0.0 | . 1 5715565 | 1 | -1.0 | . 2 5715565 | 1 | -2.0 | . 3 5715565 | 0 | -3.0 | . 4 5715565 | 1 | -4.0 | . Feature engineering . Ok, now that data is ready, let&#39;s see what featuretools have to offer. . Create entity set . First we need to create entity set - list all dataframes and their ids. We&#39;ll create an index for bureau balance data because it doesn&#39;t have one and it is required by featuretools. There&#39;s an inbuilt plotting function to check entity set before proceeding. . es = ft.EntitySet(id=&#39;credit_data&#39;) es = es.entity_from_dataframe(entity_id=&#39;applications&#39;, dataframe=app_df_reduced, index=&#39;SK_ID_CURR&#39;) es = es.entity_from_dataframe(entity_id=&#39;bureau&#39;, dataframe=bureau_df_proc, index=&#39;SK_ID_BUREAU&#39;) es = es.entity_from_dataframe(entity_id=&#39;bureau_balance&#39;, dataframe=bureau_bal_df_proc, index=&#39;SK_ID_BUREAU_BAL&#39;, make_index=True) es.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; credit_data applications applications (49650 rows) SK_ID_CURR : index AMT_ANNUITY : numeric DAYS_BIRTH : numeric DAYS_EMPLOYED : numeric DAYS_REGISTRATION : numeric DAYS_ID_PUBLISH : numeric EXT_SOURCE_1 : numeric EXT_SOURCE_2 : numeric EXT_SOURCE_3 : numeric DAYS_LAST_PHONE_CHANGE : numeric bureau bureau (232611 rows) SK_ID_BUREAU : index SK_ID_CURR : numeric CREDIT_ACTIVE : numeric CREDIT_CURRENCY : numeric CREDIT_TYPE : numeric DAYS_CREDIT : numeric CREDIT_DAY_OVERDUE : numeric DAYS_CREDIT_ENDDATE : numeric DAYS_ENDDATE_FACT : numeric AMT_CREDIT_MAX_OVERDUE : numeric CNT_CREDIT_PROLONG : numeric AMT_CREDIT_SUM : numeric AMT_CREDIT_SUM_DEBT : numeric AMT_CREDIT_SUM_LIMIT : numeric AMT_CREDIT_SUM_OVERDUE : numeric DAYS_CREDIT_UPDATE : numeric AMT_ANNUITY : numeric bureau_balance bureau_balance (2224269 rows) SK_ID_BUREAU_BAL : index SK_ID_BUREAU : numeric STATUS : numeric MONTHS_BALANCE : numeric Create relationships . Once we have our entity set, we need to establish relationships between entities (tables/dataframes). As it was shown (see diagram in the beginning of the notebook) application data is the main table, which links to bureau data via SK_ID_CURR column. Each SK_ID_CURR can have multiple records in bureau table, which has SK_ID_BUREAU unique identifier that subsequently links to records in bureau_balance table. . rel_app_bureau = ft.Relationship(parent_variable=es[&#39;applications&#39;][&#39;SK_ID_CURR&#39;], child_variable=es[&#39;bureau&#39;][&#39;SK_ID_CURR&#39;]) rel_bureau_bal = ft.Relationship(parent_variable=es[&#39;bureau&#39;][&#39;SK_ID_BUREAU&#39;], child_variable=es[&#39;bureau_balance&#39;][&#39;SK_ID_BUREAU&#39;]) es = es.add_relationships([rel_app_bureau, rel_bureau_bal]) es.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; credit_data applications applications (49650 rows) SK_ID_CURR : index AMT_ANNUITY : numeric DAYS_BIRTH : numeric DAYS_EMPLOYED : numeric DAYS_REGISTRATION : numeric DAYS_ID_PUBLISH : numeric EXT_SOURCE_1 : numeric EXT_SOURCE_2 : numeric EXT_SOURCE_3 : numeric DAYS_LAST_PHONE_CHANGE : numeric bureau bureau (232611 rows) SK_ID_BUREAU : index SK_ID_CURR : id CREDIT_ACTIVE : numeric CREDIT_CURRENCY : numeric CREDIT_TYPE : numeric DAYS_CREDIT : numeric CREDIT_DAY_OVERDUE : numeric DAYS_CREDIT_ENDDATE : numeric DAYS_ENDDATE_FACT : numeric AMT_CREDIT_MAX_OVERDUE : numeric CNT_CREDIT_PROLONG : numeric AMT_CREDIT_SUM : numeric AMT_CREDIT_SUM_DEBT : numeric AMT_CREDIT_SUM_LIMIT : numeric AMT_CREDIT_SUM_OVERDUE : numeric DAYS_CREDIT_UPDATE : numeric AMT_ANNUITY : numeric bureau&#45;&gt;applications SK_ID_CURR bureau_balance bureau_balance (2224269 rows) SK_ID_BUREAU_BAL : index SK_ID_BUREAU : id STATUS : numeric MONTHS_BALANCE : numeric bureau_balance&#45;&gt;bureau SK_ID_BUREAU Create features . Automating feature creation is as simple as calling a on-liner with established entity set and pointing to a dataframe, where features should be added. Depending on data size, entity set complexity, chosen primitives, transforms and depth (see more on https://featuretools.alteryx.com/en/stable/getting_started/afe.html) this might take a while to run. . feat_mat, feat_def = ft.dfs(entityset=es, target_entity=&#39;applications&#39;, n_jobs=-1, max_depth=2) . EntitySet scattered to 4 workers in 10 seconds . inspect_df(feat_mat) . df shape: (49650, 272) _____________________ datatypes: float32 155 float64 117 dtype: int64 _____________________ Num null vals: 5567429 _____________________ _____________________ . AMT_ANNUITY DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 DAYS_LAST_PHONE_CHANGE COUNT(bureau) ... SUM(bureau_balance.bureau.AMT_CREDIT_SUM_OVERDUE) SUM(bureau_balance.bureau.CNT_CREDIT_PROLONG) SUM(bureau_balance.bureau.CREDIT_ACTIVE) SUM(bureau_balance.bureau.CREDIT_CURRENCY) SUM(bureau_balance.bureau.CREDIT_DAY_OVERDUE) SUM(bureau_balance.bureau.CREDIT_TYPE) SUM(bureau_balance.bureau.DAYS_CREDIT) SUM(bureau_balance.bureau.DAYS_CREDIT_ENDDATE) SUM(bureau_balance.bureau.DAYS_CREDIT_UPDATE) SUM(bureau_balance.bureau.DAYS_ENDDATE_FACT) . SK_ID_CURR . 100002 24700.5 | -9461.0 | -637.0 | -3648.0 | -2120.0 | 0.083037 | 0.262949 | 0.139376 | -1134.0 | 8.0 | ... | 0.0 | 0.0 | 20.0 | 0.0 | 0.0 | 58.0 | -109646.0 | -40752.0 | -69516.0 | -72756.0 | . 100010 42075.0 | -18850.0 | -449.0 | -4597.0 | -2379.0 | 0.000000 | 0.714279 | 0.540654 | -1070.0 | 2.0 | ... | 0.0 | 0.0 | 36.0 | 0.0 | 0.0 | 180.0 | -139644.0 | -8604.0 | -41616.0 | -40968.0 | . 100017 28966.5 | -14086.0 | -3028.0 | -643.0 | -4911.0 | 0.000000 | 0.566907 | 0.770087 | -4.0 | 6.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 100021 13500.0 | -9776.0 | -191.0 | -4143.0 | -2427.0 | 0.000000 | 0.683513 | 0.000000 | -2811.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 100030 11074.5 | -19334.0 | -3494.0 | -2419.0 | -2893.0 | 0.561948 | 0.651406 | 0.461482 | 0.0 | 6.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 272 columns . Featuretools created more than 200 features out of our entity set with default primitive and transform configurations, of course not all of them make sense or add signal, so we have to perform selection again. . Modeling . Just to see if this tool added some benefit, we&#39;ll run already introduced feature selection and see if we improved the score of benchmark model. . Benchmark model . feat_mat.head().reset_index() . SK_ID_CURR AMT_ANNUITY DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 DAYS_LAST_PHONE_CHANGE ... SUM(bureau_balance.bureau.AMT_CREDIT_SUM_OVERDUE) SUM(bureau_balance.bureau.CNT_CREDIT_PROLONG) SUM(bureau_balance.bureau.CREDIT_ACTIVE) SUM(bureau_balance.bureau.CREDIT_CURRENCY) SUM(bureau_balance.bureau.CREDIT_DAY_OVERDUE) SUM(bureau_balance.bureau.CREDIT_TYPE) SUM(bureau_balance.bureau.DAYS_CREDIT) SUM(bureau_balance.bureau.DAYS_CREDIT_ENDDATE) SUM(bureau_balance.bureau.DAYS_CREDIT_UPDATE) SUM(bureau_balance.bureau.DAYS_ENDDATE_FACT) . 0 100002 | 24700.5 | -9461.0 | -637.0 | -3648.0 | -2120.0 | 0.083037 | 0.262949 | 0.139376 | -1134.0 | ... | 0.0 | 0.0 | 20.0 | 0.0 | 0.0 | 58.0 | -109646.0 | -40752.0 | -69516.0 | -72756.0 | . 1 100010 | 42075.0 | -18850.0 | -449.0 | -4597.0 | -2379.0 | 0.000000 | 0.714279 | 0.540654 | -1070.0 | ... | 0.0 | 0.0 | 36.0 | 0.0 | 0.0 | 180.0 | -139644.0 | -8604.0 | -41616.0 | -40968.0 | . 2 100017 | 28966.5 | -14086.0 | -3028.0 | -643.0 | -4911.0 | 0.000000 | 0.566907 | 0.770087 | -4.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 100021 | 13500.0 | -9776.0 | -191.0 | -4143.0 | -2427.0 | 0.000000 | 0.683513 | 0.000000 | -2811.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 100030 | 11074.5 | -19334.0 | -3494.0 | -2419.0 | -2893.0 | 0.561948 | 0.651406 | 0.461482 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 273 columns . feat_mat_proc = DataPreparator(id_cols=[&#39;SK_ID_CURR&#39;], add_rand_cols=True).prepare_data(feat_mat.reset_index()) inspect_df(feat_mat_proc) . df shape: (49650, 274) _____________________ datatypes: float32 272 int64 1 float64 1 dtype: int64 _____________________ Num null vals: 0 _____________________ _____________________ . SK_ID_CURR AMT_ANNUITY DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3 DAYS_LAST_PHONE_CHANGE ... SUM(bureau_balance.bureau.CNT_CREDIT_PROLONG) SUM(bureau_balance.bureau.CREDIT_ACTIVE) SUM(bureau_balance.bureau.CREDIT_CURRENCY) SUM(bureau_balance.bureau.CREDIT_DAY_OVERDUE) SUM(bureau_balance.bureau.CREDIT_TYPE) SUM(bureau_balance.bureau.DAYS_CREDIT) SUM(bureau_balance.bureau.DAYS_CREDIT_ENDDATE) SUM(bureau_balance.bureau.DAYS_CREDIT_UPDATE) SUM(bureau_balance.bureau.DAYS_ENDDATE_FACT) rand_number . 0 100002 | 24700.5 | -9461.0 | -637.0 | -3648.0 | -2120.0 | 0.083037 | 0.262949 | 0.139376 | -1134.0 | ... | 0.0 | 20.0 | 0.0 | 0.0 | 58.0 | -109646.0 | -40752.0 | -69516.0 | -72756.0 | 0.374540 | . 1 100010 | 42075.0 | -18850.0 | -449.0 | -4597.0 | -2379.0 | 0.000000 | 0.714279 | 0.540654 | -1070.0 | ... | 0.0 | 36.0 | 0.0 | 0.0 | 180.0 | -139644.0 | -8604.0 | -41616.0 | -40968.0 | 0.950714 | . 2 100017 | 28966.5 | -14086.0 | -3028.0 | -643.0 | -4911.0 | 0.000000 | 0.566907 | 0.770087 | -4.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.731994 | . 3 100021 | 13500.0 | -9776.0 | -191.0 | -4143.0 | -2427.0 | 0.000000 | 0.683513 | 0.000000 | -2811.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.598658 | . 4 100030 | 11074.5 | -19334.0 | -3494.0 | -2419.0 | -2893.0 | 0.561948 | 0.651406 | 0.461482 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.156019 | . 5 rows × 274 columns . feat_mat_imp = FeatureSelector(feat_mat_proc, y, id_cols=[&#39;SK_ID_CURR&#39;], rand_cols=[&#39;rand_object&#39;, &#39;rand_number&#39;]).select_important_features() . Model score with full feature set 0.6526888217522658 Number of features with greater than random column importance 8 Model score with reduced feature set 0.6556294058408862 . Conclusion . Added features didn&#39;t seem to improve the score pre and post selection by importance. This of course does not mean that the tool is useless, because most of the time feature engineering is just adding the same basic primitives and transforms - counts, sums, means, etc. Remember, we have not considered all tables available in the data, perhaps they contain more signal. Also, featuretools were run with default presets, which could be tinkered with, so definitely looks like something to add in the toolset, to inrease productivity, especially when building PoCs. .",
            "url": "https://marloz.github.io/projects/feature%20tools/feature%20engineering/automl/2020/11/06/featuretools-exploration.html",
            "relUrl": "/feature%20tools/feature%20engineering/automl/2020/11/06/featuretools-exploration.html",
            "date": " • Nov 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Dimensionality reduction and feature selection",
            "content": "Feature selection . The purpose of this post is to explore feature selection techniques, namely: . * Correlation based * K-best * In-built features importance of Random Forest * Permutation importances . Data exploration . The dataset is taken from Kaggle: https://www.kaggle.com/c/santander-customer-transaction-prediction . Load and inspect data, check dimensions, data types, missing values, target variable . import pandas as pd import numpy as np . df_train = pd.read_csv(&#39;../input/train.csv&#39;) print(df_train.shape) df_train.head() . (200000, 202) . ID_code target var_0 var_1 var_2 var_3 var_4 var_5 var_6 var_7 ... var_190 var_191 var_192 var_193 var_194 var_195 var_196 var_197 var_198 var_199 . 0 train_0 | 0 | 8.9255 | -6.7863 | 11.9081 | 5.0930 | 11.4607 | -9.2834 | 5.1187 | 18.6266 | ... | 4.4354 | 3.9642 | 3.1364 | 1.6910 | 18.5227 | -2.3978 | 7.8784 | 8.5635 | 12.7803 | -1.0914 | . 1 train_1 | 0 | 11.5006 | -4.1473 | 13.8588 | 5.3890 | 12.3622 | 7.0433 | 5.6208 | 16.5338 | ... | 7.6421 | 7.7214 | 2.5837 | 10.9516 | 15.4305 | 2.0339 | 8.1267 | 8.7889 | 18.3560 | 1.9518 | . 2 train_2 | 0 | 8.6093 | -2.7457 | 12.0805 | 7.8928 | 10.5825 | -9.0837 | 6.9427 | 14.6155 | ... | 2.9057 | 9.7905 | 1.6704 | 1.6858 | 21.6042 | 3.1417 | -6.5213 | 8.2675 | 14.7222 | 0.3965 | . 3 train_3 | 0 | 11.0604 | -2.1518 | 8.9522 | 7.1957 | 12.5846 | -1.8361 | 5.8428 | 14.9250 | ... | 4.4666 | 4.7433 | 0.7178 | 1.4214 | 23.0347 | -1.2706 | -2.9275 | 10.2922 | 17.9697 | -8.9996 | . 4 train_4 | 0 | 9.8369 | -1.4834 | 12.8746 | 6.6375 | 12.2772 | 2.4486 | 5.9405 | 19.2514 | ... | -1.4905 | 9.5214 | -0.1508 | 9.1942 | 13.2876 | -1.5121 | 3.9267 | 9.5031 | 17.9974 | -8.8104 | . 5 rows × 202 columns . def check_missing(df): return df.isnull().sum().sum() print(f&#39;Missing values {check_missing(df_train)}&#39;) . Missing values 0 . df_train.dtypes.value_counts() . float64 200 object 1 int64 1 dtype: int64 . df_train.target.sum() / df_train.shape[0] . 0.10049 . Quite large dataset with 200 numerical features, no missing values and all variable names anonymyzed, target class is inbalanced. . Sampling . In real life scenarios we would want to keep as much information as possible, given resource constraints and probably use model weights instead of downsampling. However, for the purpose of this post, let&#39;s downsample majority class to achieve 1:1 ratio, which will both speed up exploration and simplify accuracy measurement. . # Drop ID column as it doesn&#39;t contain information df_train.drop(&#39;ID_code&#39;, axis=1, inplace=True) . df_positive_class = df_train[df_train.target == 1] df_negative_class = df_train[df_train.target == 0].sample(df_positive_class.shape[0], replace=False, random_state=42) sample_df = pd.concat([df_positive_class, df_negative_class], axis=0) sample_df.target.value_counts() . 1 20098 0 20098 Name: target, dtype: int64 . Separate X and y, add random feature for importance reference, change dtypes to float32 to speedup processing . np.random.seed(42) X = (sample_df.drop(&#39;target&#39;, axis=1) .apply(lambda x: x.astype(&#39;float32&#39;)) .assign(rand=lambda x: np.random.rand(x.shape[0]))) y = sample_df.target . Create train test splits . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42) . The benchmark . Typical framework for such experiments is building the simplest solution first and then trying different techniques to improve it. Let&#39;s use the usual Random Forest classifier, which is the go-to choice for prototyping, idea testing and experimentation. We&#39;ll fit it on all features to get the benchmark and then try to reduce the number of features while maintaining as high accuracy as possible. Besides accuracy score, let&#39;s keep track of prediction time - a pretend scenario for when real time speed is important, this will add more motivation for dimensionality reduction. Remember, simple = good :) . import time def score_and_time(model, X): ts = time.time() preds = model.predict(X) elapsed = int((time.time() - ts) * 1000) print(f&#39;Score {model.oob_score_}, predicted in {elapsed}&#39;) . from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix baseline = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=-1, random_state=42) baseline.fit(X_train, y_train) score_and_time(baseline, X_test) . Score 0.7065244433387238, predicted in 209 . In-built feature importance . Random Forest was chosen for another nice property - it has in-built feature importance feature after fitting it, which saves considerable amount of coding. Let&#39;s use already fitted benchmark to get the important features. . importances = pd.Series(baseline.feature_importances_, index=X_train.columns) important_cols = importances[importances &gt; importances[&#39;rand&#39;]].index.tolist() print(f&#39;Number of features with greater than random column importance {len(important_cols)}&#39;) importances.sort_values().plot() . Number of features with greater than random column importance 178 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f11ad0854a8&gt; . reduced_rf = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=-1, random_state=42) reduced_rf.fit(X_train[important_cols], y_train) score_and_time(reduced_rf, X_test[important_cols]) . Score 0.7095720860803583, predicted in 205 . Ok, so we improved the score somewhat, prediction time is similar, dimensionality is still quite high, around 2/3 of the original input. Of course, if this was the issue, we could try sacrificing some of the accuracy and choosing less top features, instead of taking all non-random. Also, importance plot shows exponential decay, so 20/80 principle is applicable here and shouldn&#39;t hurt accuracy too much. . Importance based on correlation to target . Another way to perform variable selection that is more traditional and related to linear models is by picking features that are most correlated to target variable. Since most of the relationships in real world are not linear, we&#39;ll use Spearman&#39;s rank correlation instead of Pearson&#39;s correlation, because it&#39;s more robust and better captures non-linearities. Also, even though pandas has in-built correlation method, let&#39;s use scipy&#39;s implementation, since it&#39;s more efficient. . Some practitioners suggest investigating relationships between features first, because multicollinearity has an effect on importances, i.e. correlated variables will both seem important but contain nearly identical information. But let&#39;s leave it for now, because it would unnecessarily complicate implementation if we wanted to automate this process. . import scipy.stats _ = pd.concat([X_train,y_train], axis=1) cor = pd.DataFrame(np.abs(scipy.stats.spearmanr(_).correlation), columns=_.columns, index=_.columns)[&#39;target&#39;] non_rand_corr = cor[cor &gt; cor[&#39;rand&#39;]].shape[0] print(f&#39;Number of variables with correlation to target higher than random {non_rand_corr}&#39;) cor[cor.index != &#39;target&#39;].sort_values().plot() . Number of variables with correlation to target higher than random 199 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f11ae06b588&gt; . Most of the variables have higher absolute correlation to target than random column, so this heuristic doesn&#39;t provide a good variable selection strategy. Also all variables have very low correlations to target, e.g. 10% at most, so it&#39;s not evident how many top variables to select or what correlation threshold to set. Let&#39;s try taking top 20% most correlated variables and see how the score changes. . top20_cols = cor[cor.index != &#39;target&#39;].sort_values()[-int(cor.shape[0] * .2):].index.tolist() rf_corr = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=-1, random_state=42) rf_corr.fit(X_train[top20_cols], y_train) print(f&#39;Reduced number of columns {len(top20_cols)}&#39;) score_and_time(rf_corr, X_test[top20_cols]) . Reduced number of columns 40 Score 0.7037877845503172, predicted in 104 . Ok, so the score decreased slightly, but we reduced input dimension by 80% and prediction time twice! Of course here top 20 was chosen arbitrarily, we could do a GridSearch approach and get multiple scores for different number of top variables and see how does the trade-off landscape between accuracy and complexity looks like. . In general, in-built selection is preferrable, because importances are calculated directly when fitting trees, so it saves computation and additional code lines, but in general it&#39;s good to know alternative methods for later use with algorithms that don&#39;t have in-built feature importance calculation. . Permutation importance . The final technique that I&#39;m going to consider in this post is permutation importance, which as argued (https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py) is superior to default impurity based feature importance. . Additionally, I&#39;ll wrap feature selection part into Sklearn pipeline, such that it can be integrated into end-to-end solution. . fit base estimator, in next step get fitted estimator and apply feature importances . # scikit-learn==0.22.2 or higher required from sklearn.inspection import permutation_importance from sklearn.base import BaseEstimator, ClassifierMixin class FeatureSelector(BaseEstimator, ClassifierMixin): # This can be tuned to accept kwargs for pi def __init__(self, estimator): self.estimator = estimator def fit(self, X, y=None): self.estimator.fit(X, y) self.important_cols = self._get_important_cols(X, y) self.estimator.fit(X[self.important_cols], y) return self def _get_important_cols(self, X, y): pi = permutation_importance(self.estimator, X, y, n_repeats=1, n_jobs=-1, random_state=42) importances = pd.DataFrame(pi.importances_mean, index=X.columns, columns=[&#39;imp&#39;])[&#39;imp&#39;] return importances[importances &gt; importances[&#39;rand&#39;]].index.tolist() def predict(self, X): return self.estimator.predict(X[self.important_cols]) @property def oob_score_(self): return self.estimator.oob_score_ . from sklearn.pipeline import Pipeline clf = RandomForestClassifier(n_estimators=100, oob_score=True, n_jobs=-1, random_state=42) fs = FeatureSelector(clf) . fs.fit(X_train, y_train) print(f&#39;Number of selected features {len(fs.important_cols)}&#39;) score_and_time(fs, X_test) . Number of selected features 21 Score 0.6743065057843015, predicted in 105 . Conclusion . As you can see using permutation importances didn&#39;t seem to prove improve accuracy. It reduced the number of columns down to 21, which seems a little low. It could be due to correlated inputs, for more info see here: https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py. Anyway, it was interesting to implement new Scikit Learn methods, although for practicality I would use in-built importances for dimensionality reduction and then use permutation importances or LIME / SHAP for model interpretation. Additionally, it would be worthwile to play around with automated multicollinearity reduction techniques in the future. .",
            "url": "https://marloz.github.io/projects/sklearn/pipeline/feature%20selection/dimensionality%20reduction/2020/04/11/feature-selection.html",
            "relUrl": "/sklearn/pipeline/feature%20selection/dimensionality%20reduction/2020/04/11/feature-selection.html",
            "date": " • Apr 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Categorical preprocessing",
            "content": "import pandas as pd import numpy as np . Load and inspect data . The purpose of this notebook is not to get to the leaderboard, but rather explore various categorical preprocessing techniques, while choosing some heuristics for other modelling decisions. . I will examine the following categorical encoding techniques: . one hot | ordinal | target | . df = pd.read_csv(&#39;../input/home-credit-default-risk/application_train.csv&#39;) . df.shape . (307511, 122) . df.dtypes.value_counts() . float64 65 int64 41 object 16 dtype: int64 . df.head() . SK_ID_CURR TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY CNT_CHILDREN AMT_INCOME_TOTAL AMT_CREDIT AMT_ANNUITY ... FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR . 0 100002 | 1 | Cash loans | M | N | Y | 0 | 202500.0 | 406597.5 | 24700.5 | ... | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | . 1 100003 | 0 | Cash loans | F | N | N | 0 | 270000.0 | 1293502.5 | 35698.5 | ... | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 100004 | 0 | Revolving loans | M | Y | Y | 0 | 67500.0 | 135000.0 | 6750.0 | ... | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 100006 | 0 | Cash loans | F | N | Y | 0 | 135000.0 | 312682.5 | 29686.5 | ... | 0 | 0 | 0 | 0 | NaN | NaN | NaN | NaN | NaN | NaN | . 4 100007 | 0 | Cash loans | M | N | Y | 0 | 121500.0 | 513000.0 | 21865.5 | ... | 0 | 0 | 0 | 0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 5 rows × 122 columns . import pylab import matplotlib.pyplot as plt def plot_missing_values(df): &quot;&quot;&quot; For each column with missing values plot proportion that is missing.&quot;&quot;&quot; data = [(col, df[col].isnull().sum() / len(df)) for col in df.columns if df[col].isnull().sum() &gt; 0] col_names = [&#39;column&#39;, &#39;percent_missing&#39;] missing_df = pd.DataFrame(data, columns=col_names).sort_values(&#39;percent_missing&#39;) pylab.rcParams[&#39;figure.figsize&#39;] = (15, 8) missing_df.plot(kind=&#39;barh&#39;, x=&#39;column&#39;, y=&#39;percent_missing&#39;); plt.title(&#39;Percent of missing values in colummns&#39;); . plot_missing_values(df) . f&#39;Total missing values: {df.isnull().sum().sum()}&#39; . &#39;Total missing values: 9152465&#39; . df.TARGET.value_counts() . 0 282686 1 24825 Name: TARGET, dtype: int64 . Ok, so it&#39;s not a terribly large dataset, with mixed variable types, considerable amount of missing values and imbalanced target classes. . Sampling . First of all, I&#39;d like to downsample dominant target class to reduce imbalance. Typically, we don&#39;t need 1:1 ratio of positive to negative class, because that will most likely result in considerable information loss, however, I&#39;ll use it here to speed up development. . def downsample_df(df, target_col): positive_class = df[df[target_col] == 1] negative_class = (df[df[target_col] == 0] .sample(n=positive_class.shape[0], random_state=42)) return pd.concat([positive_class, negative_class], axis=0) . sampled_df = downsample_df(df, &#39;TARGET&#39;) . print(sampled_df.shape) sampled_df.TARGET.value_counts() . (49650, 122) . 1 24825 0 24825 Name: TARGET, dtype: int64 . Split train and validation sets . from sklearn.model_selection import train_test_split . y = sampled_df.TARGET X = sampled_df.drop(&#39;TARGET&#39;, axis=1) . X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.2, random_state=42) X_train.shape, X_valid.shape, y_train.shape, y_valid.shape . ((39720, 121), (9930, 121), (39720,), (9930,)) . Establish baseline . Any fruitful experimentation must begin by first setting a reasonable baseline result and then trying to beat it. So let&#39;s contruct a basic pipeline that provides us the benchmark, while also simplifying experimentation. . Preprocessing . Before we can get our first score on the validation set, we need to make sure that the data is in the right format to be ingested by the model, i.e. all columns should be numericalized and have no missing values. . For dealing with missing values let&#39;s just simply fill categorical NAs with &#39;none&#39; value and numericals with median. More elaborate schemes for missing value imputation are not in the scope of this notebook and can be found here: https://marloz.github.io/projects/sklearn/pipeline/missing/preprocessing/2020/03/20/sklearn-pipelines-missing-values.html . For conversion of categoricals into numerics, I&#39;ll use OneHotEncoder first, as it has in-built functionality of handling unseen classes, when applying pipeline to out-of-sample data. . def get_categorical_columns(X): return [col for col in X.columns if X[col].dtypes == &#39;O&#39;] . from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder cat_imputer = SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;none&#39;) ohe = OneHotEncoder(sparse=False, handle_unknown=&#39;ignore&#39;) cat_processor = Pipeline([ (&#39;imputer&#39;, cat_imputer), (&#39;encoder&#39;, ohe) ]) num_imputer = SimpleImputer(strategy=&#39;median&#39;) preprocessor = ColumnTransformer([(&#39;categoricals&#39;, cat_processor, get_categorical_columns)], remainder=num_imputer) . _ = pd.DataFrame(preprocessor.fit_transform(X_train)) print(f&#39;Number of missing values after imputatation {_.isnull().sum().sum()}&#39;) print(f&#39;All data types are numeric: {sum(_.dtypes == float) == _.shape[1]}&#39;) . Number of missing values after imputatation 0 All data types are numeric: True . Model . For modelling let&#39;s bring out yer good &#39;ol RandomForestClassifier, as it&#39;s very widely used, requires little preprocessing and performs well with very little tuning, which is just perfect for such exploration. . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(random_state=42, n_jobs=-1) pipeline = Pipeline([ (&#39;preprocessor&#39;, preprocessor), (&#39;classifier&#39;, rf) ]) . pipeline.fit(X_train, y_train) pipeline.score(X_valid, y_valid) . 0.6756294058408862 . Okay, so this gives us a plain vanilla pipeline score, let&#39;s see if we can squeeze out some improvement! . # Combine rare categories | . One of the simpler techniques that we can start with is combining rare classes, to avoid creating a bunch of sparse OHE columns, which doesn&#39;t help the model (for more see here https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769) . Let&#39;s first check how many variables have rare categories as this may not be issue at all in this case. There are only few variables that have categories with counts &lt; 100, so probably this technique will not add much. . min_count_categories = [(col, X_train[col].value_counts().min()) for col in get_categorical_columns(X_train)] pd.DataFrame(min_count_categories).set_index(0)[1].sort_values().plot(kind=&#39;bar&#39;) plt.yscale(&#39;log&#39;) . Also it&#39;s worth doing cardinality check, as noted in the beginning of the section, categorical variables with many levels can cause issues for tree based algorithms, especially when One Hot Encoding is applied. There&#39;s only a single variable, which has close to 60 categories, which is not too bad, so One Hot Encoding might actually be quite suitable in this case. . unique_counts = [(col, X_train[col].nunique()) for col in get_categorical_columns(X_train)] pd.DataFrame(unique_counts).set_index(0)[1].sort_values().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9e2978b518&gt; . Let&#39;s still apply rare category combiner, just to see how it works and have it in the toolbox. . def combine_low_count_classes(X, threshold=100): &quot;&quot;&quot; This is a transformer function inteded to be used on categorical columns without any missing values. It loops through variables and checks for categories that have related counts lower than specified threshold. Then it combines all these low count categories into single &#39;other&#39; category, along with first category above the threshold&quot;&quot;&quot; X = pd.DataFrame(X) for column in X.columns: frequencies = X[column].value_counts() if frequencies.min() &lt; threshold: lower_bound = frequencies[frequencies &gt;= threshold].min() mask = frequencies[frequencies &lt;= lower_bound].index replace_dict = dict.fromkeys(mask, &#39;other&#39;) X[column] = X[column].replace(replace_dict) return X.values . from sklearn.preprocessing import FunctionTransformer combiner = FunctionTransformer(combine_low_count_classes) . cat_processor = Pipeline([ (&#39;imputer&#39;, cat_imputer), (&#39;combiner&#39;, combiner), (&#39;encoder&#39;, ohe) ]) pipeline.set_params(**{&#39;preprocessor__categoricals&#39;: cat_processor}); . pipeline.fit(X_train, y_train) pipeline.score(X_valid, y_valid) . 0.6717019133937563 . Ok, this actually made the score slightly worse. As argued above, rare categories and high cardinality is probably not the main concern for this dataset, so this technique removes more information than noise. . Ordinal encoding . Ordinal encoder doesn&#39;t handle unseen data, so the pipeline will fail on validation set. To accomodate for this we need to replace new categories in validation set with some value and append this replaced value to the known classes of fitted encoder (inspired by https://stackoverflow.com/questions/40321232/handling-unknown-values-for-label-encoding) . from sklearn.base import BaseEstimator, TransformerMixin from sklearn.preprocessing import OrdinalEncoder class CustomLabelEncoder(BaseEstimator, TransformerMixin): &quot;&quot;&quot; Fits sklearn&#39;s OrdinalEncoder (OE) to categorical columns without missing values. Loops through columns and checks if each category is in within fitted OE categories. If not, new category is assigned &#39;new_category&#39; value and appended to OE categories, such that OE can be applied to unseen data.&quot;&quot;&quot; def fit(self, X, y=None): self.oe = OrdinalEncoder() self.oe.fit(X) return self def transform(self, X): for col_idx in range(X.shape[1]): X[:, col_idx] = self.replace_new_categories(X[:, col_idx], self.oe.categories_[col_idx]) self.oe.categories_[col_idx] = np.append(self.oe.categories_[col_idx], &#39;new_category&#39;) return self.oe.transform(X) def replace_new_categories(self, col, categories): return pd.Series(col).map(lambda current_category: &#39;new_category&#39; if current_category not in categories else current_category).values . cat_processor = Pipeline([ (&#39;imputer&#39;, cat_imputer), (&#39;encoder&#39;, CustomLabelEncoder()) ]) pipeline.set_params(**{&#39;preprocessor__categoricals&#39;: cat_processor}); . pipeline.fit(X_train, y_train) pipeline.score(X_valid, y_valid) . 0.6748237663645519 . No improvement from the benchmark, but scores are pretty close. . Target encoding . Another popular way of dealing with categoricals is Target Encoding, which basically includes information about the mean of the dependent variable as it relates to particular category. We have to be careful though not to leak data from validation set and also consider the number of values per category as some small groups can introduce noise (a concept akin to combining rare classes discussed before). See more here (https://medium.com/@venkatasai.katuru/target-encoding-done-the-right-way-b6391e66c19f) . class TargetEncoder(BaseEstimator, TransformerMixin): &quot;&quot;&quot; Calculates global mean on train set, then proceeds with creating encoding dictionary for each column, which contains smoothed target variable mean for each category. The smoothing parameter can be changed upon initiation of the class and represents number of values per category, i.e. for rare categories global mean has more weight. Transform part uses same trick as customized Ordinal Encoder used above - checking for new categories in data and assigning them global mean. &quot;&quot;&quot; def __init__(self, smooth_weight=100): self.smooth_weight = smooth_weight def fit(self, X, y): self.global_mean = np.mean(y) self.enc_dict = self.create_encoding_dict(X, y) return self def create_encoding_dict(self, X, y): enc_dict = {} for col_idx in range(X.shape[1]): enc_dict[col_idx] = self.get_smooth_means_for_col(X[:, col_idx], y) return enc_dict def get_smooth_means_for_col(self, col, y): smooth_mean_agg = (lambda x: (x[&#39;count&#39;] * x[&#39;mean&#39;] + self.smooth_weight * self.global_mean) / (x[&#39;count&#39;] + self.smooth_weight)) col_y_concat = pd.concat([pd.Series(col, name=&#39;col&#39;), pd.Series(y, name=&#39;target&#39;)], axis=1) return (col_y_concat.groupby(&#39;col&#39;)[&#39;target&#39;].agg([&#39;count&#39;, &#39;mean&#39;]) .assign(smooth_mean=smooth_mean_agg))[&#39;smooth_mean&#39;].to_dict() def transform(self, X): for col_idx in range(X.shape[1]): X[:, col_idx] = self.replace_new_categories(X[:, col_idx], self.enc_dict[col_idx].keys()) X[:, col_idx] = (pd.Series(X[:, col_idx]).map(self.enc_dict[col_idx]) .fillna(self.global_mean).values) return X def replace_new_categories(self, col, categories): return pd.Series(col).map(lambda current_category: &#39;new_category&#39; if current_category not in categories else current_category).values . cat_processor = Pipeline([ (&#39;imputer&#39;, cat_imputer), (&#39;encoder&#39;, TargetEncoder()) ]) pipeline.set_params(**{&#39;preprocessor__categoricals&#39;: cat_processor}); . pipeline.fit(X_train, y_train) pipeline.score(X_valid, y_valid) . 0.6780463242698892 . Slight improvement from benchmark, which is always nice. . Thanks for reading, comments are appreciated! .",
            "url": "https://marloz.github.io/projects/sklearn/pipeline/preprocessing/categorical/2020/04/10/categorical-preprocessing-techniques.html",
            "relUrl": "/sklearn/pipeline/preprocessing/categorical/2020/04/10/categorical-preprocessing-techniques.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Missing value imputation using Sklearn pipelines",
            "content": "Sklearn pipelines: missing values . In this post I&#39;ll explore missing value imputation techniques and how to combine them in a Sklearn pipeline. For this purpose I&#39;ll be using Kaggle&#39;s House Prices prediction competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques. . The goal is not to achieve maximum possible score but to demonstrate various imputation techniques and their implementation. Also I will not perform any feature engineering and do the bare minimum exploration, as we&#39;d like to have as much of the pipeline automated and generalized for other problems. . Load packages and data . Instead of doing all the package imports in the beginning of the file, I&#39;ll simply load the basic packages first and then all the rest at the moment of usage. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import pylab import seaborn as sns . It&#39;s always a good idea to look at the data first . df = pd.read_csv(&#39;/kaggle/input/house-prices-advanced-regression-techniques/train.csv&#39;) df.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 | 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 | 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 | 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 | 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 | 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 81 columns . print(df.shape) df.dtypes . (1460, 81) . Id int64 MSSubClass int64 MSZoning object LotFrontage float64 LotArea int64 ... MoSold int64 YrSold int64 SaleType object SaleCondition object SalePrice int64 Length: 81, dtype: object . df.describe().round() . Id MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 ... WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold SalePrice . count | 1460.0 | 1460.0 | 1201.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1452.0 | 1460.0 | ... | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | . mean | 730.0 | 57.0 | 70.0 | 10517.0 | 6.0 | 6.0 | 1971.0 | 1985.0 | 104.0 | 444.0 | ... | 94.0 | 47.0 | 22.0 | 3.0 | 15.0 | 3.0 | 43.0 | 6.0 | 2008.0 | 180921.0 | . std | 422.0 | 42.0 | 24.0 | 9981.0 | 1.0 | 1.0 | 30.0 | 21.0 | 181.0 | 456.0 | ... | 125.0 | 66.0 | 61.0 | 29.0 | 56.0 | 40.0 | 496.0 | 3.0 | 1.0 | 79443.0 | . min | 1.0 | 20.0 | 21.0 | 1300.0 | 1.0 | 1.0 | 1872.0 | 1950.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2006.0 | 34900.0 | . 25% | 366.0 | 20.0 | 59.0 | 7554.0 | 5.0 | 5.0 | 1954.0 | 1967.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 5.0 | 2007.0 | 129975.0 | . 50% | 730.0 | 50.0 | 69.0 | 9478.0 | 6.0 | 5.0 | 1973.0 | 1994.0 | 0.0 | 384.0 | ... | 0.0 | 25.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 6.0 | 2008.0 | 163000.0 | . 75% | 1095.0 | 70.0 | 80.0 | 11602.0 | 7.0 | 6.0 | 2000.0 | 2004.0 | 166.0 | 712.0 | ... | 168.0 | 68.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 8.0 | 2009.0 | 214000.0 | . max | 1460.0 | 190.0 | 313.0 | 215245.0 | 10.0 | 9.0 | 2010.0 | 2010.0 | 1600.0 | 5644.0 | ... | 857.0 | 547.0 | 552.0 | 508.0 | 480.0 | 738.0 | 15500.0 | 12.0 | 2010.0 | 755000.0 | . 8 rows × 38 columns . It&#39;s quite small dataset with 1460 rows, but has a lot of features - both numeric and categorical. The dependent variable is SalePrice, which we aim to predict. Right from the initial exploration we can notice some features with missing values, let&#39;s vizualize them next. . Vizualize missing values . I will use a small utility plotting function, that could be reused in other projects as well. . def plot_missing_values(df): &quot;&quot;&quot; For each column with missing values plot proportion that is missing.&quot;&quot;&quot; data = [(col, df[col].isnull().sum() / len(df)) for col in df.columns if df[col].isnull().sum() &gt; 0] col_names = [&#39;column&#39;, &#39;percent_missing&#39;] missing_df = pd.DataFrame(data, columns=col_names).sort_values(&#39;percent_missing&#39;) pylab.rcParams[&#39;figure.figsize&#39;] = (15, 8) missing_df.plot(kind=&#39;barh&#39;, x=&#39;column&#39;, y=&#39;percent_missing&#39;); plt.title(&#39;Percent of missing values in colummns&#39;); . plot_missing_values(df) . There definitely are features with missing values in this dataset, but they vary greatly in the level of missingness - from just a few to nearly all. It could be that reasons for missingness are different - PoolQC, GarageYrBlt, BsmntFinType are probably missing because not all houses have pools, garages and basements. Whereas LotFrontage could be missing because of data quality. In real life problems it is essential to figure out the reasons for missingness, but for the sake of this post let&#39;s just focus on the imputation techniques. . There are myriad of ways how to deal with missing data, for example: . Removing rows with missing values; | Removing features with high proportion of missing values; | Replacing missing values with a constant value; | Replacing missing values with mean/median/mode (globally or grouped/clustered); | Imputing missing values using models. | . In this post, I will explore the last 3 options, since the first 2 are quite trivial and, because it&#39;s a small dataset, we want to keep as much data as possible. . Constant value imputation . As a first step, let&#39;s try to replace missing values with some constants and establish a baseline. Since we have mixed datatypes, we first need to separate into categorical and numerical columns, for this I will write a custom selector transformer that will conveniently integrate into the rest of the pipeline and allow seamless preprocessing of the test set. . from sklearn.base import BaseEstimator, TransformerMixin class ColumnSelector(BaseEstimator, TransformerMixin): def __init__(self, dtype): self.dtype = dtype def fit(self, X, y=None): &quot;&quot;&quot; Get either categorical or numerical columns on fit. Store as attribute for future reference&quot;&quot;&quot; X = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X) if self.dtype == &#39;numerical&#39;: self.cols = X.select_dtypes(exclude=&#39;O&#39;).columns.tolist() elif self.dtype == &#39;categorical&#39;: self.cols = X.select_dtypes(include=&#39;O&#39;).columns.tolist() self.col_idx = [df.columns.get_loc(col) for col in self.cols] return self def transform(self, X): &quot;&quot;&quot; Subset columns of chosen data type and return np.array&quot;&quot;&quot; X = X.values if isinstance(X, pd.DataFrame) else X return X[:, self.col_idx] . Let&#39;s combine selector and imputer for numerical and categorical columns into single pipeline and check the results. For imputation I will use Sklearn&#39;s SimpleImputer. This might seem as an overkill, as it might as well be achieved using simple .fillna() method from pandas, however, we are going to be working with pipelines and move towards more complicated methods later, where usefulness of these transformers will shine, just trust me :) . from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.impute import SimpleImputer num_pipe = Pipeline([ (&#39;num_selector&#39;, ColumnSelector(&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=0)) ]) cat_pipe = Pipeline([ (&#39;cat_selector&#39;, ColumnSelector(&#39;categorical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;None&#39;)) ]) preproc = FeatureUnion([ (&#39;num_pipe&#39;, num_pipe), (&#39;cat_pipe&#39;, cat_pipe) ]) . Let&#39;s check if the output has any missing values. . def check_missing(X): print(&#39;Number of missing values after imputation: {}&#39;. format(pd.DataFrame(X).isnull().sum().sum())) . imputed_res = preproc.fit_transform(df) check_missing(imputed_res) . Number of missing values after imputation: 0 . As expected, no missing values after imputation. Inspect plots for a chosen categorical and numeric variable with missing values to get a better intuition on what was done here. . def get_df_from_pipeline_res(pipeline_res, pipeline): &quot;&quot;&quot; Get pandas dataframe from the results of fitted pipeline&quot;&quot;&quot; num_cols = pipeline.get_params()[&#39;num_pipe&#39;][&#39;num_selector&#39;].cols cat_cols = pipeline.get_params()[&#39;cat_pipe&#39;][&#39;cat_selector&#39;].cols return pd.DataFrame(pipeline_res, columns=num_cols + cat_cols) . _ = get_df_from_pipeline_res(imputed_res, preproc) f, ax = plt.subplots(1, 2) ax = ax.flatten() _.PoolQC.value_counts().plot(kind=&#39;bar&#39;, ax=ax[0]); ax[0].title.set_text(&#39;PoolQC category count after imputation&#39;); _.LotFrontage.plot(kind=&#39;kde&#39;, ax=ax[1]); ax[1].title.set_text(&#39;LotFrontage density after imputation&#39;); . As expected, SimpleImputer did it&#39;s job and added &#39;None&#39; category to categorical variables and 0 value for numeric missing values. . Establish baseline result . As always in machine learning, it&#39;s best to make minimum required number of assumptions and test, if decisions actually improve the predictions. Similarly in this case, because using constant imputation is the simplest approach, let&#39;s get the model score, consider it a benchmark and then try out more sophisticated techniques to improve upon it. . For this I will use default RandomForestRegressor with 100 trees. First separate X and y. . y = df.SalePrice X = df.drop(&#39;SalePrice&#39;, axis=1) . Since the model expects all the features to be numerical, we need to deal with the categorical columns, let&#39;s add OneHotEncoder to the pipeline. . from sklearn.preprocessing import OneHotEncoder preproc.get_params()[&#39;cat_pipe&#39;].steps.append([ &#39;ohe&#39;, OneHotEncoder(sparse=False, handle_unknown=&#39;ignore&#39;) ]) . Set an instance of simple Random Forest model. . from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor(n_estimators=100, random_state=42, oob_score=True, n_jobs=-1) . Set up the pipeline combining preporcessing and modelling steps. . estimator = Pipeline([ (&#39;preproc&#39;, preproc), (&#39;model&#39;, model) ]) . Create RMSLE scorer as target for optimization. . from sklearn.metrics import make_scorer, mean_squared_log_error def rmsle(y, y_pred): return -np.sqrt(mean_squared_log_error(y, y_pred)) scoring = make_scorer(rmsle, greater_is_better=False) . Set up grid with 5-fold cross-validation. . from sklearn.model_selection import GridSearchCV param_grid = {} grid = GridSearchCV(estimator, param_grid, scoring=scoring, n_jobs=-1, cv=5) grid.fit(X, y); . Get the benchmark result for RMSE and it&#39;s standard deviation from cross validation results. . benchmark = grid.cv_results_[&#39;mean_test_score&#39;], grid.cv_results_[&#39;std_test_score&#39;] benchmark . (array([0.14783474]), array([0.00659506])) . Our baseline model has a cross-validated RMSLE around .15, which for totally plain vanilla model is fairly ok. Let&#39;s see if we can beat this just by adding more sophisticated missing value imputation techniques. . Median / Most frequent replacement . Next, let&#39;s try median and most_frequent imputation strategies. It means that the imputer will consider each feature separately and estimate median for numerical columns and most frequent value for categorical columns. It should be stressed that both must be estimated on the training set, otherwise it will cause data leakage and poor generalization. Luckily, pipelines ensure this, even when performing cross validation. . Also be sure to include the indicator for missing values, as it adds some information for the algorithm in cases, where the chosen imputation strategy was not entirely appropriate. For example, PoolQC has the most frequent value Gd, which will replaces NA values with strategy set to most_frequent, but that is obviously wrong, as most houses don&#39;t have a pool! Hence having an indicator partially offsets this introduced noise. . estimator.get_params() . {&#39;memory&#39;: None, &#39;steps&#39;: [(&#39;preproc&#39;, FeatureUnion(n_jobs=None, transformer_list=[(&#39;num_pipe&#39;, Pipeline(memory=None, steps=[(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], verbose=False)), (&#39;cat_pipe&#39;, Pipeline(memory=None, steps=[(&#39;cat_selector&#39;, ColumnSelector(dtype...orical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], verbose=False))], transformer_weights=None, verbose=False)), (&#39;model&#39;, RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=True, random_state=42, verbose=0, warm_start=False))], &#39;verbose&#39;: False, &#39;preproc&#39;: FeatureUnion(n_jobs=None, transformer_list=[(&#39;num_pipe&#39;, Pipeline(memory=None, steps=[(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], verbose=False)), (&#39;cat_pipe&#39;, Pipeline(memory=None, steps=[(&#39;cat_selector&#39;, ColumnSelector(dtype...orical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], verbose=False))], transformer_weights=None, verbose=False), &#39;model&#39;: RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=True, random_state=42, verbose=0, warm_start=False), &#39;preproc__n_jobs&#39;: None, &#39;preproc__transformer_list&#39;: [(&#39;num_pipe&#39;, Pipeline(memory=None, steps=[(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], verbose=False)), (&#39;cat_pipe&#39;, Pipeline(memory=None, steps=[(&#39;cat_selector&#39;, ColumnSelector(dtype=&#39;categorical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], verbose=False))], &#39;preproc__transformer_weights&#39;: None, &#39;preproc__verbose&#39;: False, &#39;preproc__num_pipe&#39;: Pipeline(memory=None, steps=[(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], verbose=False), &#39;preproc__cat_pipe&#39;: Pipeline(memory=None, steps=[(&#39;cat_selector&#39;, ColumnSelector(dtype=&#39;categorical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], verbose=False), &#39;preproc__num_pipe__memory&#39;: None, &#39;preproc__num_pipe__steps&#39;: [(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], &#39;preproc__num_pipe__verbose&#39;: False, &#39;preproc__num_pipe__num_selector&#39;: ColumnSelector(dtype=&#39;numerical&#39;), &#39;preproc__num_pipe__num_imputer&#39;: SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0), &#39;preproc__num_pipe__num_selector__dtype&#39;: &#39;numerical&#39;, &#39;preproc__num_pipe__num_imputer__add_indicator&#39;: False, &#39;preproc__num_pipe__num_imputer__copy&#39;: True, &#39;preproc__num_pipe__num_imputer__fill_value&#39;: 0, &#39;preproc__num_pipe__num_imputer__missing_values&#39;: nan, &#39;preproc__num_pipe__num_imputer__strategy&#39;: &#39;constant&#39;, &#39;preproc__num_pipe__num_imputer__verbose&#39;: 0, &#39;preproc__cat_pipe__memory&#39;: None, &#39;preproc__cat_pipe__steps&#39;: [(&#39;cat_selector&#39;, ColumnSelector(dtype=&#39;categorical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], &#39;preproc__cat_pipe__verbose&#39;: False, &#39;preproc__cat_pipe__cat_selector&#39;: ColumnSelector(dtype=&#39;categorical&#39;), &#39;preproc__cat_pipe__cat_imputer&#39;: SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0), &#39;preproc__cat_pipe__ohe&#39;: OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False), &#39;preproc__cat_pipe__cat_selector__dtype&#39;: &#39;categorical&#39;, &#39;preproc__cat_pipe__cat_imputer__add_indicator&#39;: False, &#39;preproc__cat_pipe__cat_imputer__copy&#39;: True, &#39;preproc__cat_pipe__cat_imputer__fill_value&#39;: &#39;None&#39;, &#39;preproc__cat_pipe__cat_imputer__missing_values&#39;: nan, &#39;preproc__cat_pipe__cat_imputer__strategy&#39;: &#39;constant&#39;, &#39;preproc__cat_pipe__cat_imputer__verbose&#39;: 0, &#39;preproc__cat_pipe__ohe__categorical_features&#39;: None, &#39;preproc__cat_pipe__ohe__categories&#39;: None, &#39;preproc__cat_pipe__ohe__drop&#39;: None, &#39;preproc__cat_pipe__ohe__dtype&#39;: numpy.float64, &#39;preproc__cat_pipe__ohe__handle_unknown&#39;: &#39;ignore&#39;, &#39;preproc__cat_pipe__ohe__n_values&#39;: None, &#39;preproc__cat_pipe__ohe__sparse&#39;: False, &#39;model__bootstrap&#39;: True, &#39;model__criterion&#39;: &#39;mse&#39;, &#39;model__max_depth&#39;: None, &#39;model__max_features&#39;: &#39;auto&#39;, &#39;model__max_leaf_nodes&#39;: None, &#39;model__min_impurity_decrease&#39;: 0.0, &#39;model__min_impurity_split&#39;: None, &#39;model__min_samples_leaf&#39;: 1, &#39;model__min_samples_split&#39;: 2, &#39;model__min_weight_fraction_leaf&#39;: 0.0, &#39;model__n_estimators&#39;: 100, &#39;model__n_jobs&#39;: -1, &#39;model__oob_score&#39;: True, &#39;model__random_state&#39;: 42, &#39;model__verbose&#39;: 0, &#39;model__warm_start&#39;: False} . grid.param_grid = param_grid = { &#39;preproc__num_pipe__num_imputer__strategy&#39;: [&#39;median&#39;], &#39;preproc__num_pipe__num_imputer__add_indicator&#39;: [True], &#39;preproc__cat_pipe__cat_imputer__strategy&#39;: [&#39;most_frequent&#39;], &#39;preproc__cat_pipe__cat_imputer__add_indicator&#39;: [True], } grid.fit(X, y) grid.cv_results_[&#39;mean_test_score&#39;], grid.cv_results_[&#39;std_test_score&#39;] . (array([0.14654323]), array([0.00808575])) . Score improved very marginally. In real life scenarios we would like to inspect further, for which features most_frequent and median imputation is more appropriate than just simply setting to None and zero. Let&#39;s illustrate this for two variables already discussed before to get a better feel for what&#39;s happening under the hood. . # Prepare data for plotting, checking same 2 variables as before tmp_preproc = dict(dict(grid.best_estimator_.get_params()[&#39;steps&#39;])[&#39;preproc&#39;].transformer_list) lot_frontage_imputed = tmp_preproc[&#39;num_pipe&#39;][&#39;num_imputer&#39;].fit_transform(X[[&#39;LotFrontage&#39;]])[:, 0] pool_qc_imputed = tmp_preproc[&#39;cat_pipe&#39;][&#39;cat_imputer&#39;].fit_transform(X[[&#39;PoolQC&#39;]])[:, 0] res = np.vstack((lot_frontage_imputed, X[&#39;LotFrontage&#39;].fillna(0), pool_qc_imputed, X[&#39;PoolQC&#39;].fillna(&#39;None&#39;))).T cols = [&#39;lot_frontage_imputed&#39;, &#39;lot_frontage_zero_fill&#39;, &#39;pool_qc_imputed&#39;, &#39;pool_qc_const_fill&#39;] _ = pd.DataFrame(res, columns=cols) _.head() . lot_frontage_imputed lot_frontage_zero_fill pool_qc_imputed pool_qc_const_fill . 0 | 65 | 65 | Gd | None | . 1 | 80 | 80 | Gd | None | . 2 | 68 | 68 | Gd | None | . 3 | 60 | 60 | Gd | None | . 4 | 84 | 84 | Gd | None | . # Plot distributions for both variables after both methods fig, ax = plt.subplots(1, 2) ax = ax.flatten() _.lot_frontage_imputed.plot(kind=&#39;kde&#39;, ax=ax[0]) _.lot_frontage_zero_fill.plot(kind=&#39;kde&#39;, ax=ax[0]) ax[0].legend(labels=[&#39;median imputation&#39;,&#39;zero fill&#39;]) ax[0].set_title(&#39;LotFrontage distributions after different imputation methods&#39;) ax[1] = sns.countplot(data=pd.melt(_[[&#39;pool_qc_imputed&#39;, &#39;pool_qc_const_fill&#39;]]), x=&#39;value&#39;, hue=&#39;variable&#39;) ax[1].legend(labels=[&#39;most frequent imputation&#39;,&#39;constant fill&#39;]) ax[1].set_title(&#39;PoolQC distributions after different imputation methods&#39;); . If LotFrontage is truly missing and each house has it, then median imputation looks much better than zero fill. However, for PoolQC most frequent is not a valid method as it fills with Gd quality as default, in cases where actually there&#39;s no pool, as argued above. . Iterative imputation of numerical features . Next we&#39;re going to use IterativeImputer, which is still in Sklearn&#39;s experimental stage: https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html It&#39;s using iterative multivariate regression to impute missing values. . We&#39;ll built a custom transfomer that performs the whole imputation process in the following sequence: . Create mask for values to be iteratively imputed (in cases where &gt; 50% values are missing, use constant fill). | Replace all missing values with constants (None for categoricals and zeroes for numericals). | Apply ordinal encoder to numericalize categorical values, store encoded values. | Use previously created mask to fill back NaN values before iterative imputation. | Apply iterative imputer using KNeighborsRegressor as estimator. | Convert back from imputed numerical values to categorical values, by inverting fitted ordinal encoder. | . Phew! This sounds quite complicated, let&#39;s see if it improves the result. . from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import OrdinalEncoder class CustomImputer(BaseEstimator, TransformerMixin): def __init__(self, n_neighbors=5, weights=&#39;uniform&#39;, algorithm=&#39;ball_tree&#39;): self.n_neighbors=n_neighbors self.weights=weights self.algorithm=algorithm def fit(self, X, y=None): self.get_column_metadata(X) return self def get_column_metadata(self, X): &quot;&quot;&quot; Fit column selector, get names and indices for categorical and numerical columns.&quot;&quot;&quot; self.cat_cols = ColumnSelector(&#39;categorical&#39;).fit(X).cols self.num_cols = ColumnSelector(&#39;numerical&#39;).fit(X).cols def transform(self, X): &quot;&quot;&quot; Takes in X dataframe of unprocessed features and returns dataframe without missing values, either imputed using MICE method or constant imputation, depending on proportion of missing values in a column.&quot;&quot;&quot; X = X.copy() impute_mask = self.get_impute_mask(X) X_no_nan = self.replace_nan_const(X) X_cat_numericalized = self.apply_ordinal_encoder(X_no_nan[self.cat_cols]) X_numericalized = np.hstack((X_cat_numericalized, X_no_nan[self.num_cols])) X_fill_back_nan = self.fill_back_nan(X_numericalized, impute_mask) X_imputed = self.apply_imputer(X_fill_back_nan) return pd.DataFrame(self.invert_cat_encoder(X_imputed), columns=self.cat_cols + self.num_cols) def get_impute_mask(self, X): &quot;&quot;&quot; Get boolean mask marking value locations that need to be iteratively imputed. Only impute those columns, where proportion of missing values is &lt;50%. Otherwise leave constant imputation.&quot;&quot;&quot; cols_most_values_missing = [col for col in X.columns if X[col].isnull().sum() / X.shape[0] &gt; .5] impute_mask = X.isnull() impute_mask[cols_most_values_missing] = False return impute_mask def replace_nan_const(self, X): &quot;&quot;&quot; Use fitted ColumnSelector to get categorical and numerical column names. Fill missing values with &#39;None&#39; and zero constants accordingly.&quot;&quot;&quot; X[self.cat_cols] = X[self.cat_cols].fillna(&#39;None&#39;) X[self.num_cols] = X[self.num_cols].fillna(0) return X def apply_ordinal_encoder(self, X_no_nan_cat): &quot;&quot;&quot; Apply OrdinalEncoder to categorical columns, to get integer values for all categories, including missing. Make encoder available on class scope, for inversion later.&quot;&quot;&quot; self.ordinal_encoder = OrdinalEncoder() X_cat_inverted = self.ordinal_encoder.fit_transform(X_no_nan_cat) return X_cat_inverted def fill_back_nan(self, X_numericalized, impute_mask): &quot;&quot;&quot; Replace back constant values with nan&#39;s, according to imputation mask.&quot;&quot;&quot; X_numericalized[impute_mask] = np.nan return X_numericalized def apply_imputer(self, X_fill_back_nan): &quot;&quot;&quot; Use IterativeImputer to predict missing values.&quot;&quot;&quot; imputer = IterativeImputer(estimator=KNeighborsRegressor(n_neighbors=self.n_neighbors, weights=self.weights, algorithm=self.algorithm), random_state=42) return imputer.fit_transform(X_fill_back_nan) def invert_cat_encoder(self, X_imputed): &quot;&quot;&quot; Invert ordinal encoder to get back categorical values&quot;&quot;&quot; X_cats = X_imputed[:, :len(self.cat_cols)].round() X_cat_inverted = self.ordinal_encoder.inverse_transform(X_cats) X_numerics = X_imputed[:, len(self.cat_cols):] return np.hstack((X_cat_inverted, X_numerics)) . Create custom One-Hot-Encoder for categorical features, that uses categorical column names from already fitted CustomImputer. . from sklearn.compose import ColumnTransformer class CustomEncoder(BaseEstimator, TransformerMixin): def fit(self, X, y=None): ohe = OneHotEncoder(sparse=False, handle_unknown=&#39;ignore&#39;) ohe.fit(X) cat_cols = imputer[&#39;imputer&#39;].cat_cols self.ct = ColumnTransformer([(&#39;ohe&#39;, ohe, cat_cols)], remainder=&#39;passthrough&#39;) self.ct.fit(X) return self def transform(self, X): return self.ct.transform(X) . Create pipeline that applies CustomImputer and CustomEncoder in separate steps. . imputer = Pipeline([ (&#39;imputer&#39;, CustomImputer()) ]) preproc = Pipeline([ (&#39;imputer&#39;, imputer), (&#39;encoder&#39;, CustomEncoder()) ]) . Check the outpout of new preprocessor. . preproc_res = preproc.fit_transform(X) print(preproc_res.shape, check_missing(preproc_res)) pd.DataFrame(preproc_res).head() . Number of missing values after imputation: 0 (1460, 304) None . 0 1 2 3 4 5 6 7 8 9 ... 294 295 296 297 298 299 300 301 302 303 . 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 548 | 0 | 61 | 0 | 0 | 0 | 0 | 0 | 2 | 2008 | . 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 460 | 298 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 2007 | . 2 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 608 | 0 | 42 | 0 | 0 | 0 | 0 | 0 | 9 | 2008 | . 3 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 642 | 0 | 35 | 272 | 0 | 0 | 0 | 0 | 2 | 2006 | . 4 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 836 | 192 | 84 | 0 | 0 | 0 | 0 | 0 | 12 | 2008 | . 5 rows × 304 columns . estimator = Pipeline([ (&#39;preproc&#39;, preproc), (&#39;model&#39;, model) ]) grid.estimator = estimator . grid.param_grid = { &#39;preproc__imputer__imputer__n_neighbors&#39;: [3, 5, 10, 15, 20], &#39;preproc__imputer__imputer__weights&#39;: [&#39;uniform&#39;, &#39;distance&#39;] } . grid.fit(X, y) grid.cv_results_[&#39;mean_test_score&#39;], grid.cv_results_[&#39;std_test_score&#39;] . /opt/conda/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /opt/conda/lib/python3.6/site-packages/sklearn/impute/_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached. &#34; reached.&#34;, ConvergenceWarning) . (array([0.15051219, 0.15047996, 0.1494927 , 0.14943995, 0.150222 , 0.14972539, 0.14900967, 0.14833345, 0.14951359, 0.14907698]), array([0.00803959, 0.00797067, 0.00813004, 0.0083092 , 0.008971 , 0.00871621, 0.00871822, 0.00898254, 0.0074017 , 0.00782198])) . grid.best_estimator_ . Pipeline(memory=None, steps=[(&#39;preproc&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, CustomImputer(algorithm=&#39;ball_tree&#39;, n_neighbors=3, weights=&#39;uniform&#39;))], verbose=False)), (&#39;encoder&#39;, CustomEncoder())], verbose=False)), (&#39;model&#39;, RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=True, random_state=42, verbose=0, warm_start=False))], verbose=False) . Results didn&#39;t improve, because in this problem most of the variables have missing values not at random, but rather missing values having actual meaning, so doing iterative imputation only adds noise. . Anyway, it was a fun exercise to play around with the concept and hopefully it could be reused with other datasets in the future. Of course, there are other variations, tuning and estimators to play around with, but this should at least give inspiration to get started :) . Suggestions and comments are appreciated! .",
            "url": "https://marloz.github.io/projects/sklearn/pipeline/missing/preprocessing/2020/03/20/sklearn-pipelines-missing-values.html",
            "relUrl": "/sklearn/pipeline/missing/preprocessing/2020/03/20/sklearn-pipelines-missing-values.html",
            "date": " • Mar 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "“I interface with my database, my database is in cyberspace, so I’m interactive, I’m hyperactive and from time to time I’m radioactive.” . George Carlin | . Pretty much sums it up. .",
          "url": "https://marloz.github.io/projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}