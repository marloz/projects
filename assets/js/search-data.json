{
  
    
        "post0": {
            "title": "Title",
            "content": "Sklearn pipelines: missing values . In this post I&#39;ll explore missing value imputation techniques and how to combine them in a Sklearn pipeline. For this purpose I&#39;ll be using Kaggle&#39;s House Prices prediction competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques. . The goal is not to achieve maximum possible score but to demonstrate various imputation techniques and their implementation. Also I will not perform any feature engineering and do the bare minimum exploration, as we&#39;d like to have as much of the pipeline automated and generalized for other problems. . Load packages and data . Instead of doing all the package imports in the beginning of the file, I&#39;ll simply load the basic packages first and then all the rest at the moment of usage. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import pylab import seaborn as sns . It&#39;s always a good idea to look at the data first . df = pd.read_csv(&#39;/kaggle/input/house-prices-advanced-regression-techniques/train.csv&#39;) df.head() . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 | 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 | 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 | 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 | 4 | 70 | RL | 60.0 | 9550 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2006 | WD | Abnorml | 140000 | . 4 | 5 | 60 | RL | 84.0 | 14260 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 12 | 2008 | WD | Normal | 250000 | . 5 rows × 81 columns . print(df.shape) df.dtypes . (1460, 81) . Id int64 MSSubClass int64 MSZoning object LotFrontage float64 LotArea int64 ... MoSold int64 YrSold int64 SaleType object SaleCondition object SalePrice int64 Length: 81, dtype: object . df.describe().round() . Id MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 ... WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold SalePrice . count | 1460.0 | 1460.0 | 1201.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1452.0 | 1460.0 | ... | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | 1460.0 | . mean | 730.0 | 57.0 | 70.0 | 10517.0 | 6.0 | 6.0 | 1971.0 | 1985.0 | 104.0 | 444.0 | ... | 94.0 | 47.0 | 22.0 | 3.0 | 15.0 | 3.0 | 43.0 | 6.0 | 2008.0 | 180921.0 | . std | 422.0 | 42.0 | 24.0 | 9981.0 | 1.0 | 1.0 | 30.0 | 21.0 | 181.0 | 456.0 | ... | 125.0 | 66.0 | 61.0 | 29.0 | 56.0 | 40.0 | 496.0 | 3.0 | 1.0 | 79443.0 | . min | 1.0 | 20.0 | 21.0 | 1300.0 | 1.0 | 1.0 | 1872.0 | 1950.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 2006.0 | 34900.0 | . 25% | 366.0 | 20.0 | 59.0 | 7554.0 | 5.0 | 5.0 | 1954.0 | 1967.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 5.0 | 2007.0 | 129975.0 | . 50% | 730.0 | 50.0 | 69.0 | 9478.0 | 6.0 | 5.0 | 1973.0 | 1994.0 | 0.0 | 384.0 | ... | 0.0 | 25.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 6.0 | 2008.0 | 163000.0 | . 75% | 1095.0 | 70.0 | 80.0 | 11602.0 | 7.0 | 6.0 | 2000.0 | 2004.0 | 166.0 | 712.0 | ... | 168.0 | 68.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 8.0 | 2009.0 | 214000.0 | . max | 1460.0 | 190.0 | 313.0 | 215245.0 | 10.0 | 9.0 | 2010.0 | 2010.0 | 1600.0 | 5644.0 | ... | 857.0 | 547.0 | 552.0 | 508.0 | 480.0 | 738.0 | 15500.0 | 12.0 | 2010.0 | 755000.0 | . 8 rows × 38 columns . It&#39;s quite small dataset with 1460 rows, but has a lot of features - both numeric and categorical. The dependent variable is SalePrice, which we aim to predict. Right from the initial exploration we can notice some features with missing values, let&#39;s vizualize them next. . Vizualize missing values . I will use a small utility plotting function, that could be reused in other projects as well. . def plot_missing_values(df): &quot;&quot;&quot; For each column with missing values plot proportion that is missing.&quot;&quot;&quot; data = [(col, df[col].isnull().sum() / len(df)) for col in df.columns if df[col].isnull().sum() &gt; 0] col_names = [&#39;column&#39;, &#39;percent_missing&#39;] missing_df = pd.DataFrame(data, columns=col_names).sort_values(&#39;percent_missing&#39;) pylab.rcParams[&#39;figure.figsize&#39;] = (15, 8) missing_df.plot(kind=&#39;barh&#39;, x=&#39;column&#39;, y=&#39;percent_missing&#39;); plt.title(&#39;Percent of missing values in colummns&#39;); . plot_missing_values(df) . There definitely are features with missing values in this dataset, but they vary greatly in the level of missingness - from just a few to nearly all. It could be that reasons for missingness are different - PoolQC, GarageYrBlt, BsmntFinType are probably missing because not all houses have pools, garages and basements. Whereas LotFrontage could be missing because of data quality. In real life problems it is essential to figure out the reasons for missingness, but for the sake of this post let&#39;s just focus on the imputation techniques. . There are myriad of ways how to deal with missing data, for example: . Removing rows with missing values; | Removing features with high proportion of missing values; | Replacing missing values with a constant value; | Replacing missing values with mean/median/mode (globally or grouped/clustered); | Imputing missing values using models. | . In this post, I will explore the last 3 options, since the first 2 are quite trivial and, because it&#39;s a small dataset, we want to keep as much data as possible. . Constant value imputation . As a first step, let&#39;s try to replace missing values with some constants and establish a baseline. Since we have mixed datatypes, we first need to separate into categorical and numerical columns, for this I will write a custom selector transformer that will conveniently integrate into the rest of the pipeline and allow seamless preprocessing of the test set. . from sklearn.base import BaseEstimator, TransformerMixin class ColumnSelector(BaseEstimator, TransformerMixin): def __init__(self, dtype): self.dtype = dtype def fit(self, X, y=None): &quot;&quot;&quot; Get either categorical or numerical columns on fit. Store as attribute for future reference&quot;&quot;&quot; X = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X) if self.dtype == &#39;numerical&#39;: self.cols = X.select_dtypes(exclude=&#39;O&#39;).columns.tolist() elif self.dtype == &#39;categorical&#39;: self.cols = X.select_dtypes(include=&#39;O&#39;).columns.tolist() self.col_idx = [df.columns.get_loc(col) for col in self.cols] return self def transform(self, X): &quot;&quot;&quot; Subset columns of chosen data type and return np.array&quot;&quot;&quot; X = X.values if isinstance(X, pd.DataFrame) else X return X[:, self.col_idx] . Let&#39;s combine selector and imputer for numerical and categorical columns into single pipeline and check the results. For imputation I will use Sklearn&#39;s SimpleImputer. This might seem as an overkill, as it might as well be achieved using simple .fillna() method from pandas, however, we are going to be working with pipelines and move towards more complicated methods later, where usefulness of these transformers will shine, just trust me :) . from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.impute import SimpleImputer num_pipe = Pipeline([ (&#39;num_selector&#39;, ColumnSelector(&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=0)) ]) cat_pipe = Pipeline([ (&#39;cat_selector&#39;, ColumnSelector(&#39;categorical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(strategy=&#39;constant&#39;, fill_value=&#39;None&#39;)) ]) preproc = FeatureUnion([ (&#39;num_pipe&#39;, num_pipe), (&#39;cat_pipe&#39;, cat_pipe) ]) . Let&#39;s check if the output has any missing values. . def check_missing(X): print(&#39;Number of missing values after imputation: {}&#39;. format(pd.DataFrame(X).isnull().sum().sum())) . imputed_res = preproc.fit_transform(df) check_missing(imputed_res) . Number of missing values after imputation: 0 . As expected, no missing values after imputation. Inspect plots for a chosen categorical and numeric variable with missing values to get a better intuition on what was done here. . def get_df_from_pipeline_res(pipeline_res, pipeline): &quot;&quot;&quot; Get pandas dataframe from the results of fitted pipeline&quot;&quot;&quot; num_cols = pipeline.get_params()[&#39;num_pipe&#39;][&#39;num_selector&#39;].cols cat_cols = pipeline.get_params()[&#39;cat_pipe&#39;][&#39;cat_selector&#39;].cols return pd.DataFrame(pipeline_res, columns=num_cols + cat_cols) . _ = get_df_from_pipeline_res(imputed_res, preproc) f, ax = plt.subplots(1, 2) ax = ax.flatten() _.PoolQC.value_counts().plot(kind=&#39;bar&#39;, ax=ax[0]); ax[0].title.set_text(&#39;PoolQC category count after imputation&#39;); _.LotFrontage.plot(kind=&#39;kde&#39;, ax=ax[1]); ax[1].title.set_text(&#39;LotFrontage density after imputation&#39;); . As expected, SimpleImputer did it&#39;s job and added &#39;None&#39; category to categorical variables and 0 value for numeric missing values. . Establish baseline result . As always in machine learning, it&#39;s best to make minimum required number of assumptions and test, if decisions actually improve the predictions. Similarly in this case, because using constant imputation is the simplest approach, let&#39;s get the model score, consider it a benchmark and then try out more sophisticated techniques to improve upon it. . For this I will use default RandomForestRegressor with 100 trees. First separate X and y. . y = df.SalePrice X = df.drop(&#39;SalePrice&#39;, axis=1) . Since the model expects all the features to be numerical, we need to deal with the categorical columns, let&#39;s add OneHotEncoder to the pipeline. . from sklearn.preprocessing import OneHotEncoder preproc.get_params()[&#39;cat_pipe&#39;].steps.append([ &#39;ohe&#39;, OneHotEncoder(sparse=False, handle_unknown=&#39;ignore&#39;) ]) . Set an instance of simple Random Forest model. . from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor(n_estimators=100, random_state=42, oob_score=True, n_jobs=-1) . Set up the pipeline combining preporcessing and modelling steps. . estimator = Pipeline([ (&#39;preproc&#39;, preproc), (&#39;model&#39;, model) ]) . Create RMSLE scorer as target for optimization. . from sklearn.metrics import make_scorer, mean_squared_log_error def rmsle(y, y_pred): return -np.sqrt(mean_squared_log_error(y, y_pred)) scoring = make_scorer(rmsle, greater_is_better=False) . Set up grid with 5-fold cross-validation. . from sklearn.model_selection import GridSearchCV param_grid = {} grid = GridSearchCV(estimator, param_grid, scoring=scoring, n_jobs=-1, cv=5) grid.fit(X, y); . Get the benchmark result for RMSE and it&#39;s standard deviation from cross validation results. . benchmark = grid.cv_results_[&#39;mean_test_score&#39;], grid.cv_results_[&#39;std_test_score&#39;] benchmark . (array([0.14783474]), array([0.00659506])) . Our baseline model has a cross-validated RMSLE around .15, which for totally plain vanilla model is fairly ok. Let&#39;s see if we can beat this just by adding more sophisticated missing value imputation techniques. . Median / Most frequent replacement . Next, let&#39;s try median and most_frequent imputation strategies. It means that the imputer will consider each feature separately and estimate median for numerical columns and most frequent value for categorical columns. It should be stressed that both must be estimated on the training set, otherwise it will cause data leakage and poor generalization. Luckily, pipelines ensure this, even when performing cross validation. . Also be sure to include the indicator for missing values, as it adds some information for the algorithm in cases, where the chosen imputation strategy was not entirely appropriate. For example, PoolQC has the most frequent value Gd, which will replaces NA values with strategy set to most_frequent, but that is obviously wrong, as most houses don&#39;t have a pool! Hence having an indicator partially offsets this introduced noise. . estimator.get_params() . {&#39;memory&#39;: None, &#39;steps&#39;: [(&#39;preproc&#39;, FeatureUnion(n_jobs=None, transformer_list=[(&#39;num_pipe&#39;, Pipeline(memory=None, steps=[(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], verbose=False)), (&#39;cat_pipe&#39;, Pipeline(memory=None, steps=[(&#39;cat_selector&#39;, ColumnSelector(dtype...orical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], verbose=False))], transformer_weights=None, verbose=False)), (&#39;model&#39;, RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=True, random_state=42, verbose=0, warm_start=False))], &#39;verbose&#39;: False, &#39;preproc&#39;: FeatureUnion(n_jobs=None, transformer_list=[(&#39;num_pipe&#39;, Pipeline(memory=None, steps=[(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], verbose=False)), (&#39;cat_pipe&#39;, Pipeline(memory=None, steps=[(&#39;cat_selector&#39;, ColumnSelector(dtype...orical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], verbose=False))], transformer_weights=None, verbose=False), &#39;model&#39;: RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=True, random_state=42, verbose=0, warm_start=False), &#39;preproc__n_jobs&#39;: None, &#39;preproc__transformer_list&#39;: [(&#39;num_pipe&#39;, Pipeline(memory=None, steps=[(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], verbose=False)), (&#39;cat_pipe&#39;, Pipeline(memory=None, steps=[(&#39;cat_selector&#39;, ColumnSelector(dtype=&#39;categorical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], verbose=False))], &#39;preproc__transformer_weights&#39;: None, &#39;preproc__verbose&#39;: False, &#39;preproc__num_pipe&#39;: Pipeline(memory=None, steps=[(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], verbose=False), &#39;preproc__cat_pipe&#39;: Pipeline(memory=None, steps=[(&#39;cat_selector&#39;, ColumnSelector(dtype=&#39;categorical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], verbose=False), &#39;preproc__num_pipe__memory&#39;: None, &#39;preproc__num_pipe__steps&#39;: [(&#39;num_selector&#39;, ColumnSelector(dtype=&#39;numerical&#39;)), (&#39;num_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0))], &#39;preproc__num_pipe__verbose&#39;: False, &#39;preproc__num_pipe__num_selector&#39;: ColumnSelector(dtype=&#39;numerical&#39;), &#39;preproc__num_pipe__num_imputer&#39;: SimpleImputer(add_indicator=False, copy=True, fill_value=0, missing_values=nan, strategy=&#39;constant&#39;, verbose=0), &#39;preproc__num_pipe__num_selector__dtype&#39;: &#39;numerical&#39;, &#39;preproc__num_pipe__num_imputer__add_indicator&#39;: False, &#39;preproc__num_pipe__num_imputer__copy&#39;: True, &#39;preproc__num_pipe__num_imputer__fill_value&#39;: 0, &#39;preproc__num_pipe__num_imputer__missing_values&#39;: nan, &#39;preproc__num_pipe__num_imputer__strategy&#39;: &#39;constant&#39;, &#39;preproc__num_pipe__num_imputer__verbose&#39;: 0, &#39;preproc__cat_pipe__memory&#39;: None, &#39;preproc__cat_pipe__steps&#39;: [(&#39;cat_selector&#39;, ColumnSelector(dtype=&#39;categorical&#39;)), (&#39;cat_imputer&#39;, SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0)), [&#39;ohe&#39;, OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False)]], &#39;preproc__cat_pipe__verbose&#39;: False, &#39;preproc__cat_pipe__cat_selector&#39;: ColumnSelector(dtype=&#39;categorical&#39;), &#39;preproc__cat_pipe__cat_imputer&#39;: SimpleImputer(add_indicator=False, copy=True, fill_value=&#39;None&#39;, missing_values=nan, strategy=&#39;constant&#39;, verbose=0), &#39;preproc__cat_pipe__ohe&#39;: OneHotEncoder(categorical_features=None, categories=None, drop=None, dtype=&lt;class &#39;numpy.float64&#39;&gt;, handle_unknown=&#39;ignore&#39;, n_values=None, sparse=False), &#39;preproc__cat_pipe__cat_selector__dtype&#39;: &#39;categorical&#39;, &#39;preproc__cat_pipe__cat_imputer__add_indicator&#39;: False, &#39;preproc__cat_pipe__cat_imputer__copy&#39;: True, &#39;preproc__cat_pipe__cat_imputer__fill_value&#39;: &#39;None&#39;, &#39;preproc__cat_pipe__cat_imputer__missing_values&#39;: nan, &#39;preproc__cat_pipe__cat_imputer__strategy&#39;: &#39;constant&#39;, &#39;preproc__cat_pipe__cat_imputer__verbose&#39;: 0, &#39;preproc__cat_pipe__ohe__categorical_features&#39;: None, &#39;preproc__cat_pipe__ohe__categories&#39;: None, &#39;preproc__cat_pipe__ohe__drop&#39;: None, &#39;preproc__cat_pipe__ohe__dtype&#39;: numpy.float64, &#39;preproc__cat_pipe__ohe__handle_unknown&#39;: &#39;ignore&#39;, &#39;preproc__cat_pipe__ohe__n_values&#39;: None, &#39;preproc__cat_pipe__ohe__sparse&#39;: False, &#39;model__bootstrap&#39;: True, &#39;model__criterion&#39;: &#39;mse&#39;, &#39;model__max_depth&#39;: None, &#39;model__max_features&#39;: &#39;auto&#39;, &#39;model__max_leaf_nodes&#39;: None, &#39;model__min_impurity_decrease&#39;: 0.0, &#39;model__min_impurity_split&#39;: None, &#39;model__min_samples_leaf&#39;: 1, &#39;model__min_samples_split&#39;: 2, &#39;model__min_weight_fraction_leaf&#39;: 0.0, &#39;model__n_estimators&#39;: 100, &#39;model__n_jobs&#39;: -1, &#39;model__oob_score&#39;: True, &#39;model__random_state&#39;: 42, &#39;model__verbose&#39;: 0, &#39;model__warm_start&#39;: False} . grid.param_grid = param_grid = { &#39;preproc__num_pipe__num_imputer__strategy&#39;: [&#39;median&#39;], &#39;preproc__num_pipe__num_imputer__add_indicator&#39;: [True], &#39;preproc__cat_pipe__cat_imputer__strategy&#39;: [&#39;most_frequent&#39;], &#39;preproc__cat_pipe__cat_imputer__add_indicator&#39;: [True], } grid.fit(X, y) grid.cv_results_[&#39;mean_test_score&#39;], grid.cv_results_[&#39;std_test_score&#39;] . (array([0.14654323]), array([0.00808575])) . Score improved very marginally. In real life scenarios we would like to inspect further, for which features most_frequent and median imputation is more appropriate than just simply setting to None and zero. Let&#39;s illustrate this for two variables already discussed before to get a better feel for what&#39;s happening under the hood. . # Prepare data for plotting, checking same 2 variables as before tmp_preproc = dict(dict(grid.best_estimator_.get_params()[&#39;steps&#39;])[&#39;preproc&#39;].transformer_list) lot_frontage_imputed = tmp_preproc[&#39;num_pipe&#39;][&#39;num_imputer&#39;].fit_transform(X[[&#39;LotFrontage&#39;]])[:, 0] pool_qc_imputed = tmp_preproc[&#39;cat_pipe&#39;][&#39;cat_imputer&#39;].fit_transform(X[[&#39;PoolQC&#39;]])[:, 0] res = np.vstack((lot_frontage_imputed, X[&#39;LotFrontage&#39;].fillna(0), pool_qc_imputed, X[&#39;PoolQC&#39;].fillna(&#39;None&#39;))).T cols = [&#39;lot_frontage_imputed&#39;, &#39;lot_frontage_zero_fill&#39;, &#39;pool_qc_imputed&#39;, &#39;pool_qc_const_fill&#39;] _ = pd.DataFrame(res, columns=cols) _.head() . lot_frontage_imputed lot_frontage_zero_fill pool_qc_imputed pool_qc_const_fill . 0 | 65 | 65 | Gd | None | . 1 | 80 | 80 | Gd | None | . 2 | 68 | 68 | Gd | None | . 3 | 60 | 60 | Gd | None | . 4 | 84 | 84 | Gd | None | . # Plot distributions for both variables after both methods fig, ax = plt.subplots(1, 2) ax = ax.flatten() _.lot_frontage_imputed.plot(kind=&#39;kde&#39;, ax=ax[0]) _.lot_frontage_zero_fill.plot(kind=&#39;kde&#39;, ax=ax[0]) ax[0].legend(labels=[&#39;median imputation&#39;,&#39;zero fill&#39;]) ax[0].set_title(&#39;LotFrontage distributions after different imputation methods&#39;) ax[1] = sns.countplot(data=pd.melt(_[[&#39;pool_qc_imputed&#39;, &#39;pool_qc_const_fill&#39;]]), x=&#39;value&#39;, hue=&#39;variable&#39;) ax[1].legend(labels=[&#39;most frequent imputation&#39;,&#39;constant fill&#39;]) ax[1].set_title(&#39;PoolQC distributions after different imputation methods&#39;); . If LotFrontage is truly missing and each house has it, then median imputation looks much better than zero fill. However, for PoolQC most frequent is not a valid method as it fills with Gd quality as default, in cases where actually there&#39;s no pool, as argued above. . Iterative imputation of numerical features . Next we&#39;re going to use IterativeImputer, which is still in Sklearn&#39;s experimental stage: https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html It&#39;s using iterative multivariate regression to impute missing values. . We&#39;ll built a custom transfomer that performs the whole imputation process in the following sequence: . Create mask for values to be iteratively imputed (in cases where &gt; 50% values are missing, use constant fill). | Replace all missing values with constants (None for categoricals and zeroes for numericals). | Apply ordinal encoder to numericalize categorical values, store encoded values. | Use previously created mask to fill back NaN values before iterative imputation. | Apply iterative imputer using KNeighborsRegressor as estimator. | Convert back from imputed numerical values to categorical values, by inverting fitted ordinal encoder. | . Phew! This sounds quite complicated, let&#39;s see if it improves the result. . from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import OrdinalEncoder class CustomImputer(BaseEstimator, TransformerMixin): def __init__(self, n_neighbors=5, weights=&#39;uniform&#39;, algorithm=&#39;ball_tree&#39;): self.n_neighbors=n_neighbors self.weights=weights self.algorithm=algorithm def fit(self, X, y=None): self.get_column_metadata(X) return self def get_column_metadata(self, X): &quot;&quot;&quot; Fit column selector, get names and indices for categorical and numerical columns.&quot;&quot;&quot; self.cat_cols = ColumnSelector(&#39;categorical&#39;).fit(X).cols self.num_cols = ColumnSelector(&#39;numerical&#39;).fit(X).cols def transform(self, X): &quot;&quot;&quot; Takes in X dataframe of unprocessed features and returns dataframe without missing values, either imputed using MICE method or constant imputation, depending on proportion of missing values in a column.&quot;&quot;&quot; X = X.copy() impute_mask = self.get_impute_mask(X) X_no_nan = self.replace_nan_const(X) X_cat_numericalized = self.apply_ordinal_encoder(X_no_nan[self.cat_cols]) X_numericalized = np.hstack((X_cat_numericalized, X_no_nan[self.num_cols])) X_fill_back_nan = self.fill_back_nan(X_numericalized, impute_mask) X_imputed = self.apply_imputer(X_fill_back_nan) return pd.DataFrame(self.invert_cat_encoder(X_imputed), columns=self.cat_cols + self.num_cols) def get_impute_mask(self, X): &quot;&quot;&quot; Get boolean mask marking value locations that need to be iteratively imputed. Only impute those columns, where proportion of missing values is &lt;50%. Otherwise leave constant imputation.&quot;&quot;&quot; cols_most_values_missing = [col for col in X.columns if X[col].isnull().sum() / X.shape[0] &gt; .5] impute_mask = X.isnull() impute_mask[cols_most_values_missing] = False return impute_mask def replace_nan_const(self, X): &quot;&quot;&quot; Use fitted ColumnSelector to get categorical and numerical column names. Fill missing values with &#39;None&#39; and zero constants accordingly.&quot;&quot;&quot; X[self.cat_cols] = X[self.cat_cols].fillna(&#39;None&#39;) X[self.num_cols] = X[self.num_cols].fillna(0) return X def apply_ordinal_encoder(self, X_no_nan_cat): &quot;&quot;&quot; Apply OrdinalEncoder to categorical columns, to get integer values for all categories, including missing. Make encoder available on class scope, for inversion later.&quot;&quot;&quot; self.ordinal_encoder = OrdinalEncoder() X_cat_inverted = self.ordinal_encoder.fit_transform(X_no_nan_cat) return X_cat_inverted def fill_back_nan(self, X_numericalized, impute_mask): &quot;&quot;&quot; Replace back constant values with nan&#39;s, according to imputation mask.&quot;&quot;&quot; X_numericalized[impute_mask] = np.nan return X_numericalized def apply_imputer(self, X_fill_back_nan): &quot;&quot;&quot; Use IterativeImputer to predict missing values.&quot;&quot;&quot; imputer = IterativeImputer(estimator=KNeighborsRegressor(n_neighbors=self.n_neighbors, weights=self.weights, algorithm=self.algorithm), random_state=42) return imputer.fit_transform(X_fill_back_nan) def invert_cat_encoder(self, X_imputed): &quot;&quot;&quot; Invert ordinal encoder to get back categorical values&quot;&quot;&quot; X_cats = X_imputed[:, :len(self.cat_cols)].round() X_cat_inverted = self.ordinal_encoder.inverse_transform(X_cats) X_numerics = X_imputed[:, len(self.cat_cols):] return np.hstack((X_cat_inverted, X_numerics)) . Create custom One-Hot-Encoder for categorical features, that uses categorical column names from already fitted CustomImputer. . from sklearn.compose import ColumnTransformer class CustomEncoder(BaseEstimator, TransformerMixin): def fit(self, X, y=None): ohe = OneHotEncoder(sparse=False, handle_unknown=&#39;ignore&#39;) ohe.fit(X) cat_cols = imputer[&#39;imputer&#39;].cat_cols self.ct = ColumnTransformer([(&#39;ohe&#39;, ohe, cat_cols)], remainder=&#39;passthrough&#39;) self.ct.fit(X) return self def transform(self, X): return self.ct.transform(X) . Create pipeline that applies CustomImputer and CustomEncoder in separate steps. . imputer = Pipeline([ (&#39;imputer&#39;, CustomImputer()) ]) preproc = Pipeline([ (&#39;imputer&#39;, imputer), (&#39;encoder&#39;, CustomEncoder()) ]) . Check the outpout of new preprocessor. . preproc_res = preproc.fit_transform(X) print(preproc_res.shape, check_missing(preproc_res)) pd.DataFrame(preproc_res).head() . Number of missing values after imputation: 0 (1460, 304) None . 0 1 2 3 4 5 6 7 8 9 ... 294 295 296 297 298 299 300 301 302 303 . 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 548 | 0 | 61 | 0 | 0 | 0 | 0 | 0 | 2 | 2008 | . 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 460 | 298 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 2007 | . 2 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 608 | 0 | 42 | 0 | 0 | 0 | 0 | 0 | 9 | 2008 | . 3 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 642 | 0 | 35 | 272 | 0 | 0 | 0 | 0 | 2 | 2006 | . 4 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 836 | 192 | 84 | 0 | 0 | 0 | 0 | 0 | 12 | 2008 | . 5 rows × 304 columns . estimator = Pipeline([ (&#39;preproc&#39;, preproc), (&#39;model&#39;, model) ]) grid.estimator = estimator . grid.param_grid = { &#39;preproc__imputer__imputer__n_neighbors&#39;: [3, 5, 10, 15, 20], &#39;preproc__imputer__imputer__weights&#39;: [&#39;uniform&#39;, &#39;distance&#39;] } . grid.fit(X, y) grid.cv_results_[&#39;mean_test_score&#39;], grid.cv_results_[&#39;std_test_score&#39;] . /opt/conda/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak. &#34;timeout or by a memory leak.&#34;, UserWarning /opt/conda/lib/python3.6/site-packages/sklearn/impute/_iterative.py:603: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached. &#34; reached.&#34;, ConvergenceWarning) . (array([0.15051219, 0.15047996, 0.1494927 , 0.14943995, 0.150222 , 0.14972539, 0.14900967, 0.14833345, 0.14951359, 0.14907698]), array([0.00803959, 0.00797067, 0.00813004, 0.0083092 , 0.008971 , 0.00871621, 0.00871822, 0.00898254, 0.0074017 , 0.00782198])) . grid.best_estimator_ . Pipeline(memory=None, steps=[(&#39;preproc&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, Pipeline(memory=None, steps=[(&#39;imputer&#39;, CustomImputer(algorithm=&#39;ball_tree&#39;, n_neighbors=3, weights=&#39;uniform&#39;))], verbose=False)), (&#39;encoder&#39;, CustomEncoder())], verbose=False)), (&#39;model&#39;, RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=True, random_state=42, verbose=0, warm_start=False))], verbose=False) . Results didn&#39;t improve, because in this problem most of the variables have missing values not at random, but rather missing values having actual meaning, so doing iterative imputation only adds noise. . Anyway, it was a fun exercise to play around with the concept and hopefully it could be reused with other datasets in the future. Of course, there are other variations, tuning and estimators to play around with, but this should at least give inspiration to get started :) . Suggestions and comments are appreciated! .",
            "url": "https://marloz.github.io/projects/2020/03/20/sklearn-pipelines-missing-values.html",
            "relUrl": "/2020/03/20/sklearn-pipelines-missing-values.html",
            "date": " • Mar 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://marloz.github.io/projects/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://marloz.github.io/projects/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}